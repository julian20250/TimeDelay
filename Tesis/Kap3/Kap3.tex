\chapter{Estadística bayesiana}
En este capítulo se hace una breve introducción a la teoría bayesiana, con el fin de obtener las herramientas necesarias para ajustar los parámetros de un modelo dado a través de un conjunto de datos experimentales. Algunas referencias clave de este capítulo son \cite{Arunachalam,rossi_2018,speagle2019conceptual,kish_1995,brooks_gelman_jones_2011}.
\section{Preliminares}
Antes de enunciar la regla de Bayes, es necesario definir lo que es un espacio de probabilidad, y la probabilidad condicional. Esta sección busca fundamentar las bases de la teoría de la probabilidad, para comprender a cabalidad temas posteriores.
\subsection{Espacio de probabilidad}
La probabilidad es una teoría matemática que busca medir de alguna forma la posibilidad de que ocurra un evento contenido en un conjunto de posibles eventos, resultados todos de un experimento. Por supuesto, no se conoce de forma determinista cuál será el resultado tras la ejecución del experimento, de modo que sólo se puede hablar de posibilidades de que ocurra algún evento. Este tipo de experimentos se conocerán como experimentos aleatorios.

\begin{defi}[Experimento Aleatorio]
		Un experimento se dice aleatorio si su resultado no se puede determinar de antemano.
	\end{defi}
\begin{defi}[Espacio de Muestra]
		El conjunto $\Omega$ de todos los posibles resultados de un experimento aleatorio se llama espacio de muestra. Un elemento $\omega\in\Omega$ se llama resultado o muestra. $\Omega$ se dice discreto si es finito o contable.
	\end{defi}	
	Ahora se requiere definir lo que se entenderá por evento, para lo cual se definirá una estructura sobre el espacio de muestra, conocida como $\sigma-$álgebra, que dará cuenta de los eventos de interés tras la ejecución de un experimento aleatorio. 
	\begin{defi}[$\sigma$-álgebra]
		Tome $\Omega\neq\emptyset$. Una colección $\Im$ de subconjuntos de $\Omega$ se llama $\sigma-$álgebra sobre $\Omega$ si:
		\begin{enumerate}
			\item $\Omega\in\Im$,
			\item Si $A\in\Im$, entonces $A^c\in\Im$ y,
			\item Si $A_1,A_2,\dots\in\Im$, entonces $\bigcup_{i=1}^\infty A_i\in\Im$.
		\end{enumerate}
		Los elementos de $\Im$ se llaman eventos.
	\end{defi}
	El siguiente teorema será de utilidad para construir una $\sigma$-álgebra a partir de un conjunto finito o contable de $\sigma$-álgebras.
	\begin{theo}
		Si $\Omega\neq\emptyset$ y $\Im_1,\Im_2,\dots$ son $\sigma-$álgebras sobre $\Omega$, entonces $\bigcap_{i=1}^\infty \Im_i$ es una $\sigma$-álgebra sobre $\Omega$.
	\end{theo}		
	
	\begin{proof}
		Como $\Omega\in\Im_j,$ para $j=1,2,\dots$, $\Omega\in\bigcap_{j=1}^\infty \Im_j$. Si $A\in\bigcap_{j=1}^\infty\Im_j$, $A\in\Im_j$, para $j=1,2,\dots$, de modo que $A^c\in\Im_j$, y $A^c\in\bigcap_{j=1}^\infty\Im_j$. Por último, si 
		$$A_1,A_2,\dots\in\bigcap_{j=1}^\infty\Im_j,$$
		para todo $j=1,2,\dots$, $A_1,A_2,\dots\in\Im_j$, de modo que
		$$\bigcup_{i=1}^\infty A_i\in\Im_j\text{ y }\bigcup_{i=1}^\infty A_i\in\bigcap_{j=1}^\infty\Im_j.$$ 
	\end{proof}
	
	Con este teorema, se puede definir la $\sigma$-álgebra más pequeña\footnote{Es la más pequeña en el sentido de que es la que requiere menos elementos para satisfacer las condiciones necesarias para ser una $\sigma$-álgebra.} que contiene un subconjunto de $\Omega$. 	
	\begin{defi}[$\sigma$-álgebra generada]
		Tome $\Omega\neq\emptyset$ y $\mathcal{A}$ como una colección de subconjuntos de $\Omega$. Si $\mathcal{M}:=\{\Im:\Im\text{ es una }\sigma-\text{álgebra sobre }\Omega \text{ que contiene a } \mathcal{A}\},$
		$$\sigma(\mathcal{A}):=\bigcap_{\Im\in \mathcal{M}}\Im$$
		es la $\sigma-$álgebra más pequeña sobre $\Omega$ que contiene a $\mathcal{A}$. Esta $\sigma-$álgebra se conoce como $\sigma$-álgebra generada por $\mathcal{A}$.
	\end{defi}
	
	\begin{defi}[Espacio de medida]
		Tome $\Omega\neq\emptyset$ y sea $\Im$ una $\sigma$-álgebra sobre $\Omega$. La pareja $(\Omega,\Im)$ se llama espacio de medida.
	\end{defi}
	
	Al evento $\emptyset$ se le conoce como evento imposible; $\Omega$ es el evento seguro y $\{\omega\}$, con $\omega\in\Omega$ es un evento simple. Se dice que el evento $A$ ocurre después de llevar a cabo el experimento aleatorio si se obtiene un resultado en $A$, esto es, $A$ ocurre si el resultado es algún $\omega\in A$.
	\begin{enumerate}
		\item El evento $A\cup B$ ocurre si y sólo si $A$ ocurre, $B$ pasa, o ambos ocurren.
		\item El evento $A\cap B$ ocurre si y sólo si $A$ y $B$ ocurren a la vez.
		\item El evento $A^c$ ocurre si y sólo si $A$ no ocurre.
		\item El evento $A-B$ ocurre si y sólo si $A$ ocurre pero $B$ no ocurre.
	\end{enumerate}
	Si dos eventos no tienen eventos simples en común, se dirá que son eventos mutuamente excluyentes:
	\begin{defi}[Eventos mutuamente excluyentes]
		Dos eventos $A$ y $B$ se dicen mutuamente excluyentes si $A\cap B=\emptyset$.
	\end{defi}
	
	Antes de introducir la función de probabilidad, que medirá la posibilidad de que ocurra un evento de la $\sigma-$álgebra, es necesario definir por completez la frecuencia relativa, pues ella determina la posibilidad de que ocurra un evento al cabo de $n$ repeticiones del experimento aleatorio.
	\begin{defi}[Frecuencia relativa]
		Para cada evento $A$, el número $f_r(A):=\frac{n(A)}{n}$ se llama la frecuencia relativa de $A$, donde $n(A)$ indica el número de veces que ocurre $A$ en $n$ repeticiones del experimento aleatorio.
	\end{defi}
	Cuando $n\rightarrow\infty$, se puede hablar intuitivamente de la probabilidad de que ocurra el evento $A$, normalizada de $0$ a $1$. Por supuesto, es imposible realizar infinitas veces un experimento aleatorio para determinar la probabilidad de ocurrencia de todos los eventos de la $\sigma-$álgebra, por lo que se introduce de antemano la función de probabilidad, suponiendo que ella da cuenta del comportamiento de la frecuencia relativa cuando $n\rightarrow\infty$.
	
	\begin{defi}[Espacio de probabilidad]
		Tome $(\Omega,\Im)$ como un espacio de medida. Una función real $P$ sobre $\Im$ que satisface las siguientes condiciones:
		\begin{enumerate}
			\item $P(A)\geq0$ para todo $A\in\Im$ (no negativa),
			\item $P(\Omega)=1$ (normalizada) y,
			\item si $A_1,A_2,\dots$ son eventos mutuamente excluyentes en $\Im$, esto es, si
			$$A_i\cap A_j=\emptyset \text{ para todo }i\neq j, \text{ entonces}$$
			$$P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i),$$
		\end{enumerate}
		se llama medida de probabilidad sobre $(\Omega,\Im)$. La tripleta $(\Omega,\Im,P)$ se llama espacio de probabilidad.
	\end{defi}
	El siguiente teorema caracteriza las propiedades más importantes de un espacio de probabilidad.
	\begin{theo}
		Si $(\Omega,\Im,P)$ es un espacio de probabilidad, entonces
		\begin{enumerate}
			\item $P(\emptyset)=0$.
			\item Si $A,B\in\Im$ y $A\cap B=\emptyset$, entonces $P(A\cup B)=P(A)+P(B).$
			\item Para todo $A\in\Im$, $P(A^c)=1-P(A).$
			\item Si $A\subseteq B$, entonces $P(A)\leq P(B)$ y $P(B-A)=P(B)-P(A).$ En particular, $P(A)\leq1$ para todo $A\in\Im$.
			\item Para todo $A,B\in\Im$, $P(A\cup B)=P(A)+P(B)-P(A\cap B)$.
		
		\item Tome $\{A_n\}_n\subseteq\Im$ como una sucesión creciente, esto es, $A_n\subseteq A_{n+1}, \forall n\in\mathbb{N}$; entonces
			$$P\left(\lim_{m\rightarrow\infty}A_n\right)=\lim_{n\rightarrow\infty}P(A_n), \text{ donde } \lim_{n\rightarrow\infty} A_n:=\bigcup_{i=1}^\infty A_i.$$
		\item Tome $\{A_n\}_n\subseteq\Im$ como una sucesión decreciente, esto es, $A_n\supseteq A_{n+1}, \forall n\in\mathbb{N}$; entonces
			$$P\left(\lim_{m\rightarrow\infty}A_n\right)=\lim_{n\rightarrow\infty}P(A_n), \text{ donde } \lim_{n\rightarrow\infty} A_n:=\bigcap_{i=1}^\infty A_i.$$
	\end{enumerate}
	\end{theo}
	\begin{proof}
		\begin{enumerate}
			\item $1=P(\Omega\cup\emptyset\cup\emptyset\cup\cdots)=P(\Omega)+P(\emptyset)+P(\emptyset)+\cdots=1+P(\emptyset)+\cdots \implies P(\emptyset)=0.$
			\item $P(A\cup B)=P(A\cup B\cup\emptyset\cup\emptyset\cup\cdots)=P(A)+P(B).$
			\item $P(A)+P(A^c)=P(A\cup A^c)=P(\Omega)=1\implies P(A^c)=1-P(A).$
			\item Si $A\subseteq B$, $B=A\cup(B-A)$, de modo que $P(B)=P(A)+P(B-A).$ Como $P\geq0$, $P(B)\geq P(A)$ y $P(B-A)=P(B)-P(A).$ Si $B=\Omega,$ $P(A)\leq1.$
			\item Use el hecho de que $A\cup B=[A-(A\cap B)]\cup[B-(A\cap B)]\cup[A\cap B]$.			
			\item Tome la sucesión $C_1=A_1,C_2=A_2-A_1,\dots,C_r=A_r-A_{r-1},\dots$. Es claro que
			$$\bigcup_{i=1}^\infty C_i=\bigcup_{i=1}^\infty A_i.$$
			Más aún, como $C_i\cap C_j=\emptyset\ \forall i\neq j,$ se sigue que
			$$P\left(\bigcup_{i=1}^\infty A_i\right)=P\left(\bigcup_{n=1}^\infty C_n\right)=\sum_{n=1}^\infty P(C_n)=\lim_{n\rightarrow\infty} \sum_{k=1}^n P(C_k)$$ $$=\lim_{n\rightarrow\infty}P\left(\bigcup_{k=1}^n C_k\right)=\lim_{n\rightarrow\infty}P\left(A_n\right).$$
			\item Tome la sucesión $\{B_n=A_n^c\}_n$ y aplique el resultado anterior.
		\end{enumerate}
	\end{proof}
Aplicando el teorema anterior de forma inductiva, para algunos eventos $A_1,A_2,\dots,A_n\in\Im$:
	\begin{align*}
	P(A_1\cup A_2\cup \cdots\cup A_n)=&\sum_{i=1}^n P(A_i)-\sum_{i_1<i_2}P(A_{i_{1}}\cap A_{i_{2}})+\cdots \\    
			& +(-1)^{r+1}\sum_{i_1<i_2<	\cdots<i_r}P(A_{i_1}\cap A_{i_2}\cap\cdots\cap A_{i_r})\\
			&+\cdots+(-1)^{n+1}P(A_1\cap A_2\cap \cdots\cap A_n).
	\end{align*}
	
	Por otro lado, tome $(\Omega,\Im,P)$ como un espacio de probabilidad con $\Omega$ finito o contable y $\Im =\mathbb{P}(\Omega)$. Tome $\emptyset\neq A \in\Im.$ Es claro que
	
	$$A=\bigcup_{\omega\in A} \{\omega\}, \text{ de modo que }$$
	
	$$P(A)=\sum_{\omega\in A} P(\omega), \text{ donde } P(\omega):=P(\{\omega\}).$$
	Así, $P$ queda completamente definido por $p_j:=P(\omega_j)$, donde $\omega_j\in\Omega.$ El vector $|\Omega|-$dimensional $p:=(p_1,p_2,\dots)$ satisface las siguientes condiciones:
	\begin{itemize}
		\item $p_j\geq0$ y
		\item $\sum_{j=1}^\infty p_j=1.$
	\end{itemize}
	Un vector que satisface las anteriores condiciones se llama \textbf{vector de probabilidad}.
	
	\subsection{Probabilidad condicional}
	Tome $B$ como un evento cuya opción de ocurrir debe ser medida bajo la suposición de que otro evento $A$ fue observado. Si el experimento se repite $n$ veces bajo las mismas circunstancias, entonces la frecuencia relativa de $B$ bajo la condición $A$ se define como
		$$f_r(B|A):=\frac{n(A\cap B)}{n(A)}=\frac{\frac{n(A\cap B)}{n}}{\frac{n(A)}{n}}=\frac{f_r(A\cap B)}{f_r(A)}, \text{ si }n(A)>0.$$
		Esto motiva la definición de probabilidad condicional, como el comportamiento de esta frecuencia relativa cuando $n\rightarrow\infty$.
		\begin{defi}[Probabilidad condicional]
			Tome $(\Omega, \Im, P)$ como un espacio de probabilidad. Si $A,B\in\Im$, con $P(A)>0$, entonces la probabilidad del evento $B$ bajo la condición $A$ se define como sigue
			$$P(B|A):=\frac{P(A\cap B)}{P(A).}$$
		\end{defi}
		El siguiente teorema provee algunas propiedades de la probabilidad condicional.
		\begin{theo}[Medida de probabilidad condicional]
			Tome $(\Omega, \Im, P)$ como un espacio de probabilidad y $A\in\Im$, con $P(A)>0$. Entonces:
			\begin{enumerate}
				\item $P(\cdot | A)$ es una medida de probabilidad sobre $\Omega$ centrada en $A$, esto es, $P(A|A)=1.$
				\item Si $A\cap B=\emptyset$, entonces $P(B|A)=0$.
				\item $P(B\cap C |A)=P(B|A\cap C)P(C|A)$ si $P(A\cap C)>0$.
				\item Si $A_1,A_2,\dots,A_n\in\Im$, con $P(A_1\cap A_2\cap\cdots\cap A_{n-1})>0$, entonces
				\begin{align*}
				P(A_1\cap A_2\cap\cdots\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_n|A_1\cap A_2\cap \cdots\cap A_{n-1}).
				\end{align*}
			\end{enumerate}
		\end{theo}		
		
		\begin{proof}
			\begin{enumerate}
				\item Las tres propiedades de una medida de probabilidad deben ser verificadas.
				\begin{enumerate}
					\item Claramente, $P(B|A)\geq0$ para todo $B\in\Im$.
					\item $P(\Omega | A)=\frac{P(\Omega\cap A)}{P(A)}=\frac{P(A)}{P(A)}=1.$ También se tiene que $P(A|A)=1.$
					\item Tome $A_1,A_2,\dots\in\Im$ una sucesión de conjuntos disyuntos. Entonces
					\begin{align*}
					P\left(\bigcup_{i=1}^\infty A_i | A\right)=\frac{P\left(A\cap \bigcup_{i=1}^\infty A_i\right)}{P(A)}=\frac{P\left(\bigcup_{i=1}^\infty A\cap A_i\right)}{P(A)}\\
					=\sum_{i=1}^\infty \frac{P(A\cap A_i)}{P(A)}=\sum_{i=1}^\infty P(A_i | A).
					\end{align*}
				\end{enumerate}
				\item Si $A\cap B=\emptyset$, $P(B|A)=\frac{P(A\cap B)}{P(A)}=\frac{P(\emptyset)}{P(A)}=0.$
				\item $P(B\cap C|A)=\frac{P(A\cap B\cap C)}{P(A)}=\frac{P(B\cap C\cap A)}{P(A\cap C)}\frac{P(C\cap A)}{P(A)}=P(B|A\cap C)P(C| A).$
				\item $P(A_1\cap\cdots\cap A_n)=\frac{P(A_1\cap\cdots\cap A_n)}{P(A_1\cap\cdots\cap A_{n-1})}\frac{P(A_1\cap\cdots\cap A_{n-1})}{P(A_1\cap\cdots\cap A_{n-2})}\cdots\frac{P(A_1\cap A_2)}{P(A_1)}P(A_1)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_n|A_1\cap A_2\cap \cdots\cap A_{n-1}).$
			\end{enumerate}
			\end{proof}	
			El siguiente teorema permitirá probar la regla de Bayes.
			
		\begin{theo}[Teorema de probabilidad total]
			Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$, esto es, $A_i\cap A_j=\emptyset, \forall i\neq j$ y $\bigcup_{i=1}^\infty A_i=\Omega,$ tal que $P(A_i)>0$, para todo $A_i\in\Im$. Entonces, para todo $B\in\Im$:
			$$P(B)=\sum_{i} P(B|A_i)P(A_i).$$
		\end{theo}
		\begin{proof}
			Observe que
			$$B=B\cap\Omega=B\cap\left( \bigcup_{i=1}^\infty A_i \right)=\bigcup_{i=1}^\infty B\cap A_i,$$
			de modo que
			$$P(B)=P\left(\bigcup_{i=1}^\infty B\cap A_i\right)=\sum_{i=1}^\infty P(B\cap A_i)=\sum_{i=1}^\infty P(B| A_i)P(A_i).$$
		\end{proof}
		Matemáticamente, este teorema se puede interpretar como que la probabilidad de que ocurra $B$ se puede medir en términos de una partición de $\Omega$ en el sentido de que $B$, como subconjunto de $\Omega$ puede ocurrir cuando ocurran algunos elementos de la partición, los cuales tendrán mayor peso en el término $P(B|A_i)$ de la suma.
		
		Como corolario del teorema anterior, se obtiene la \textbf{regla de Bayes}, que constituye la base para la \textbf{teoría Bayesiana}.
		\begin{corol}[Regla de Bayes]
			Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$ con $P(A_i)>0$, para todo $i$; entonces, para todo $B\in\Im$ con $P(B)>0:$
			$$P(A_i | B)=\frac{P(A_i)P(B| A_i)}{\sum_j P(B|A_j)P(A_j)}, \forall i.$$
		\end{corol}
		\begin{proof}
		$$P(A_i|B)=\frac{P(A_i\cap B)}{P(B)}=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B| A_i)}{\sum_j P(B|A_j)P(A_j)}.$$
		Con la partición $A_1=A, A_2=A^c$ se obtiene la forma usual de la regla de Bayes.
	\end{proof}
	
	A continuación se definen las distribuciones a priori y a posteriori, que hacen referencia a la probabilidad de que ocurran ciertos eventos de una partición de $\Omega$ antes de que ocurra un evento $B$ y al cabo de que este evento $B$ ocurra.
	\begin{defi}[Distribuciones a priori y a posteriori]
		Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$, con $P(A_i)>0$, para todo $i$. Si $P(B)>0$, con $B\in\Im$, entonces $\{P(A_n)\}_n$ se llama distribución a priori (antes de que $B$ ocurra), y $\{P(A_n|B)\}_n$ se llama distribución a posteriori (después de que $B$ ocurra).
	\end{defi}
	Algunas veces, la ocurrencia de un evento $B$ no afecta la probabilidad de un evento $A$, es decir,
	$$P(A|B)=P(A).$$
	En este caso, se dice que el evento $A$ es independiente del evento $B$. Esto motiva la siguiente definición.
	\begin{defi}[Eventos independientes]
		Dos eventos $A$ y $B$ se dicen independientes si y sólo si
		
		$$P(A\cap B)=P(A)P(B).$$
		Si esta condición no se tiene, se dice que los eventos son dependientes.
	\end{defi}
	
	En algunos casos, es necesario analizar la independencia de dos o más eventos. Para ello, se dan las siguientes definiciones.
	\begin{defi}[Familia independiente]
		Una familia de eventos $\{A_i: i\in I\}$ se dice independiente si
		$$P\left(\bigcap_{i\in J} A_i\right)=\prod_{i\in J}P(A_i),$$
		para cualquier subconjunto no vacío $J\subseteq I$.
	\end{defi}
	\begin{defi}[Eventos independientes par a par]
		Una familia de eventos $\{A_i: i\in I\}$ se dice par a par independiente si
		$$P(A_i\cap A_j)=P(A_i)P(A_j), \text{ para todo }i\neq j.$$
	\end{defi}
	
	\subsection{Variables aleatorias}
	En un experimento aleatorio, generalmente hay mayor interés en determinar ciertos valores numéricos asociados a los resultados del experimento aleatorio, que al resultado mismo del experimento aleatorio. Con esto en mente se define la variable aleatoria.

	\begin{defi}[Variable aleatoria]
	Tome $(\Omega, \Im, P)$ como un espacio de probabilidad. Una variable aleatoria es un mapa $X:\Omega\rightarrow \mathbb{R}$ tal que, para todo $A\in\mathbb{B}$, $X^{-1}(A)\in\Im$, donde $\mathbb{B}$ es la $\sigma$-álgebra de Borel sobre $\mathbb{R}$ ($\sigma$-álgebra más pequeña que contiene todos los intervalos de la forma $(-\infty, a]$).
	
	El conjunto de posibles valores de $X$ es $\mathbb{S}:=\{x\in\mathbb{R}:\exists \omega\in\Omega\text{ tal que }X(\omega)=x\}$, conocido como \textbf{soporte de la variable aleatoria }$X$.
	\end{defi}
	Si $X$ es una variable aleatoria definida sobre un  espacio de probabilidad $(\Omega, \Im, P)$, se introduce la notación
	$$\{X\in B\}:=\{\omega\in\Omega: X(\omega)\in B\}, \text{ con }B\in\mathbb{B}.$$
	\begin{defi}[Variable aleatoria discreta]
		Una variable aleatoria $X$ se dice discreta cuando el soporte $\mathbb{S}$ de $X$ es un subconjunto finito o contable de $\mathbb{R}$. Para $x\in\mathbb{S}$, la función $f(x)=P(X=x)$ se llama función de densidad de probabilidad (pdf para abreviar).
	\end{defi}
	
	\begin{defi}[Variable aleatoria continua]
		Una variable aleatoria $X$ se dice continua si el soporte $\mathbb{S}$ de $X$ es la unión de uno o más intervalos y si existe una función no negativa y real $f(x)$ tal que $P(X\leq x)=\int_{-\infty}^x f(t)\mathrm{d}t.$ La función $f(x)$ se llama función de densidad de probabilidad (pdf).
	\end{defi}
	
	Algunas propiedades de la \textit{pdf} discreta son las siguientes:
	\begin{enumerate}
		\item $f(x)\geq0, \forall x\in\mathbb{S}$ y $f(x)=0,\forall x\not\in \mathbb{S}$.
		\item $\sum_{x\in\mathbb{S}}f(x)=1$.
		\item $P(X\in B)=\sum_{x\in B} f(x).$
	\end{enumerate}
	Análogamente, para la \textit{pdf} continua:
	\begin{enumerate}
		\item $f(x)\geq0, \forall x\in\mathbb{S}$ y $f(x)=0,\forall x\not\in \mathbb{S}$.
		\item $\int_{\mathbb{S}}f(x)\mathrm{d}x=1$.
		\item $P(X\in B)=\int_{B}f(x)\mathrm{d}x.$
	\end{enumerate}
	\begin{defi}[Función de distribución acumulativa]
		La función de distribución acumulativa (CDF, para abreviar) de una variable aleatoria se define como la función $F(x)=P(X\leq x)$.
	\end{defi}
	El siguiente teorema resume algunas propiedades importantes de una \textit{CDF}. 
	
	\begin{theo}
		Si $X$ es una variable aleatoria, con \textit{CDF} $F(x)$, entonces:
		\begin{enumerate}
			\item $\lim_{x\rightarrow-\infty}F(x)=0$ y $\lim_{x\rightarrow\infty} F(x)=1.$
			\item $F(x)$ es no decreciente; esto es, $F(x)\leq F(y)$, siempre que $x\leq y$.
			\item $F(x)$ es continua por derecha.
			\item $P(a< X \leq b)=F(b)-F(a).$
		\end{enumerate}
	\end{theo}
	
	\begin{proof}

	\begin{enumerate}
		
		\item \textbf{Cuando $\mathbf{x\rightarrow-\infty}$}, para todo $n\in\mathbb{N}$, se satisface que
		$$\{X\leq -n\} \supseteq \{X\leq -(n+1)\}, \text{ y en adición,}$$
		$$\emptyset = \bigcap_{n=1}^\infty \{ X\leq -n\}, \text{ con lo que}$$
		$$\lim_{x\rightarrow-\infty}F(x)=\lim_{n\rightarrow \infty}F(-n)=P(\emptyset)=0.$$
		
		\textbf{Cuando $\mathbf{x\rightarrow\infty}$}, para todo $n\in\mathbb{N}$, se satisface que
		$$\{X\leq n\}\subseteq\{ X\leq n+1\}, \text{ y,}$$
		$$\Omega=\bigcup_{n=1}^\infty \{ X\leq n\}, \text{ de modo que}$$
		$$\lim_{x\rightarrow\infty} F(x)=\lim_{n\rightarrow\infty}=P(\Omega)=1.$$
		
		\item Si $x\leq y$, entonces
		$$\{X\leq x\}\subseteq \{X\leq y\}, \textit{ con lo que}$$
		$$F(x)=P(X\leq x)\leq P(X\leq y)=F(y).$$
		
		\item Tome $x\in\mathbb{R}$ fijo. Suponga que $\{x_n\}_{n\in\mathbb{N}}$ es una sucesión decreciente de números reales, cuyo límite va a $x$. Se puede ver que
		$$\{X\leq x_1\}\supseteq\{X\leq x_2\}\supseteq\cdots, \text{ y}$$
		$$\bigcap_{n=1}^\infty \{X\leq x_n\}=\{ X\leq x\}, \text{ de modo que}$$
		$$\lim_{y\rightarrow x^+} F(y)=\lim_{n\rightarrow \infty} F(x_n)=\lim_{n\rightarrow\infty} P(X\leq x_n)=P(X\leq x)=F(x).$$
		
		\item Dado que $\Omega=\{X\leq a\}\cup\{a<X\leq b\}\cup \{X>b\},$
		$$1=P(X\leq a)+P(a<X\leq b)+P(X>b),$$
		$$\therefore P(a<X\leq b)=P(X\leq b)-P(X\leq a)=F(b)-F(a).$$
	\end{enumerate}
		
	\end{proof}
	
	Los teoremas posteriores indican cómo determinar la \textit{pdf} a partir de la \textit{CDF} de una variable aleatoria.
	\begin{theo}
		Si $X$ es una variable aleatoria discreta con \textit{CDF} $F(x)$ y soporte $\mathbb{S}=\{x_0,x_1,\dots\}$, con $x_0<x_1<\cdots$, entonces, para $x_k\in\mathbb{S}$,
		$$f(x_k)=F(x_k)-F(x_{k-1}).$$
	\end{theo}
	\begin{proof}
		Para $k\geq 1$,
		$$f(x_k)=P(X=x_k)=P(x_{k-1}<X\leq x_k)=F(x_k)-F(x_{k-1}).$$
	\end{proof}
	\begin{theo}
		Para una variable aleatoria continua, $f(x)=\frac{\mathrm{d}F}{\mathrm{d}x}, \forall x\in\mathbb{R}.$
	\end{theo}
	\begin{proof}
		La prueba se sigue directamente del teorema fundamental del cálculo.
	\end{proof}
	\subsection{Vectores aleatorios}
	En la mayoría de los análisis estadísticos, más de una variable debe ser analizada al cabo de un experimento aleatorio. Cada observación se puede representar como un vector de observaciones, conocido como vector aleatorio.
	
	\begin{defi}[Vector aleatorio]
		Un vector aleatorio $\vec{X}=(X_1,X_2,\dots,X_k)$ es un vector $k-$dimensional, donde $X_1,\dots,X_k$ son variables aleatorias. Un vector aleatorio se dice discreto cuando cada una de las variables aleatorias que lo conforman son discretas, y continuo cuando son continuas.
	\end{defi}
	
	\begin{defi}[Variable aleatoria bivariada]
		Un vector aleatorio bidimensional $\vec{X}=(X_1,X_2)$ se llama variable aleatoria bivariada.
	\end{defi}
	
	De modo similar al caso de las variables aleatorias, los vectores aleatorios tienen \textit{pdf}, un soporte y una \textit{CDF}. El soporte de un vector aleatorio $k-$dimensional es el conjunto de valores que puede tomar, denotado por $\mathbb{S}_{\vec{X}}\subseteq\mathbb{R}^k.$
	
	\begin{defi}[Función de densidad de probabilidad adjunta discreta]
		Tome $\vec{X}$ como un vector aleatorio discreto $k-$dimensional. La \textit{pdf} adjunta de $\vec{X}$ se define como
		$$f(\vec{x}):=f(x_1,x_2,\dots,x_k)=P(X_1=x_1,X_2=x_2,\dots, X_k=x_k)$$
		para $\vec{x}=(x_1,x_2,\dots,x_k)\in\mathbb{S}_{\vec{X}}.$
	\end{defi}
	La \textit{pdf} adjunta discreta tiene las siguientes propiedades:
	\begin{enumerate}
		\item $0\leq f(x_1,x_2,\dots,x_k)\leq1, \forall \vec{x}\in\mathbb{S}_{\vec{X}}.$
		\item $\sum_{\vec{x}\in\mathbb{S}_{\vec{X}}}f(\vec{x})=1.$
		\item Para cualquier subconjunto $B\subseteq \mathbb{S}_{\vec{X}},\ P(\vec{X}\in B)=\sum_{\{\vec{x}\in\mathbb{S}_{\vec{X}}:\vec{x}\in B\}}f(\vec{x}).$
	\end{enumerate}
	
	\begin{defi}[Función de densidad de probabilidad adjunta continua]
		Tome $\vec{X}$ como un vector aleatorio $k$-dimensional continuo. La \textit{pdf} continua de $\vec{X}$ se define como cualquier función no negativa $f(\vec{x})$ que satisfaga las siguientes propiedades:
		\begin{enumerate}
			\item $f(x_1,\dots,x_k)>0, \forall \vec{x}\in\mathbb{S}_{\vec{X}}.$
			\item $\int_{\mathbb{S}_{\vec{X}}} f(x_1,\dots,x_k)\mathrm{d}x_1\cdots\mathrm{d}x_k=1.$
			\item Para cualquier subconjunto $B\subset\mathbb{S}_{\vec{X}},\ P(\vec{X}\in B)=\int_{B}f(x_1,\dots,x_k)\mathrm{d}x_1\cdots\mathrm{d}x_k.$
		\end{enumerate}
	\end{defi}
	
	\begin{defi}[Función de distribución acumulativa adjunta]
		Tome $\vec{X}=(X_1,X_2,\dots,X_k)$ como un vector aleatorio $k-$dimensional. La \textit{CDF} adjunta de $\vec{X}$ se define como
		$$F(x_1,x_2,\dots,x_k)=P(X_1\leq x_1, X_2\leq x_2,\dots, X_k\leq x_k), \ \forall (x_1,\dots,x_k)\in\mathbb{R}^k.$$
	\end{defi}
	Las componentes de un vector aleatorio $\vec{X}$ son variables aleatorias, por lo que las \textit{pdf} de cada variable aleatoria $X_i$ de $\vec{X}$ se pueden derivar de la \textit{pdf} adjunta de $\vec{X}$.
	\begin{defi}[Función de densidad de probabilidad marginal]
		Tome $\vec{X}=(X_1,\dots,X_k)$ como un vector aleatorio $k-$dimensional. La función de densidad de probabilidad marginal de la variable aleatoria $X_i$ es
		
		\begin{center}
		\begin{tabular}{cc}
		$f_i(x_i)=\underbrace{\sum_{x_1\in\mathbb{S}_{X}}\cdots \sum_{x_k\in\mathbb{S}_{X_k}}}_{\text{quitando la suma sobre }x_i} f(x_1,\dots,x_k)$ & cuando $\vec{X}$ es discreta y\\
		$f_i(x_i)=\underbrace{\int_{x_1\in\mathbb{S}_{X_1}}\cdots \int_{x_k\in\mathbb{S}_{X_k}}}_{\text{quitando la integral sobre }x_i} f(x_1,\dots,x_k)\prod_{n\neq i}\mathrm{d}x_n$ & cuando $\vec{X}$ es continua.
		\end{tabular}
		\end{center}
	\end{defi}
	También se puede definir la distribución condicional dada una variable aleatoria de forma similar al caso de la probabilidad condicional.
	\begin{defi}[Función de densidad de probabilidad condicional]
		Tome $\vec{X}(X_1,\dots,X_k)$ como un vector aleatorio $k-$dimensional. Para un valor fijo de $x_i$, donde $f_i(x_i)>0$, la función de densidad de probabilidad condicional para $\vec{Y}|X_i$; donde $\vec{Y}$ es un vector aleatorio $(k-1)-$dimensional con todas las variables aleatorias de $\vec{X}$, a excepción de $X_i$; es
		$$f(\vec{y}|x_i)=\frac{f(x_1,\dots,x_k)}{f_i(x_i)},$$
		donde $\vec{y}\in\mathbb{S}_{\vec{Y}}.$
	\end{defi}
	
	Esta última definición motiva, como en el caso de los eventos independientes, el concepto de variables aleatorias independientes.
	
	\begin{defi}[Colección independiente de variables aleatorias]
	Una colección de variables aleatorias $\{ X_1,X_2,\dots, X_k\}$ se dice independiente cuando
	$$F(x_1,x_2,\dots,x_k)=\prod_{i=1}^k F_i(x_i), \forall\vec{x}\in\mathbb{R}^k,$$
	donde $F_i(x_i)$ es la \textit{CDF} marginal de la variable aleatoria $X_i$ (determinada a partir de la \textit{pdf} marginal: $F_i(x_i):=P(X_i\leq x_i)$).
	
	También se puede definir la independencia entre variables aleatorias usando las pdf, en el sentido de que la misma colección de variables aleatorias se dice independiente cuando
	$$f(x_1,x_2,\dots,x_k)=\prod_{i=1}^k f_i(x_i), \forall\vec{x}\in\mathbb{S}_{\vec{X}},$$
	donde $f_i(x_i)$ es la pdf marginal asociada a la variable aleatoria $X_i$.
	
	\end{defi}
	
	\begin{defi}[Colección de variables aleatorias independientes idénticamente distribuidas]
		Una colección de variables aleatorias $\{ X_1,X_2,\dots, X_k\}$ se dice independiente e idénticamente distribuidas (iid, para abreviar) si y sólo si $X_1,X_2,\dots,X_k$ son variables aleatorias independientes y la pdf de cada variable aleatoria es idéntica.
	\end{defi}
	
	\section{Función de verosimilitud}
	\subsection{Estadística}
		En la realidad, uno se encuentra en presencia de un experimento aleatorio del cual desconoce su función de densidad de probabilidad, dependiendo de las variables aleatorias asociadas a este. A través de una muestra lo suficientemente amplia, se desearía es reconstruir el modelo probabilístico que generó dicha muestra. Sin embargo, el problema resulta casi imposible de resolver sin especificar la forma de la función de densidad de probabilidad, por lo que se suele hacer es escoger una \textit{pdf} con ciertos parámetros a determinar, buscando el mejor ajuste posible respecto a la muestra. Las componentes de esta estimación paramétrica son las siguientes \cite{rossi_2018}
		\begin{enumerate}
			\item Un modelo probabilístico $f(x,\theta)$, especificado con los valores de los parámetros desconocidos.
			\item Un conjunto de posibles valores de $\theta$ bajo consideración, llamado el espacio de parámetros, denotado por $\Theta$.
			\item Una muestra aleatoria de $n$ observaciones del modelo probabilístico.
			\item Un conjunto de estimadores puntuales para los valores de los parámetros desconocidos, basados en la información contenida en la muestra aleatoria.
			\item Las propiedades específicas de los estimadores que permiten evaluar la precisión y eficiencia del estimador.
		\end{enumerate}
		A continuación se define lo que se entenderá por muestra.
		\begin{defi}[Muestra]
			Una colección de variables aleatorias $X_1,\dots,X_k$ se llama muestra de tamaño $n$. Una muestra de $n$ variables aleatorias independientes $X_1,\dots,X_n$ se llama muestra aleatoria.
		\end{defi}
%Inicio nueva parte presentación =========================		
		Con una muestra dada, se pueden estimar algunos parámetros de una población. Esta estimación se conoce como estadística.
		
		\begin{defi}[Estadística y estimador]
			Dada una muestra $X_1,X_2,\dots,X_n$, una estadística $T=T(X_1,\dots,X_n)$ es una función de la muestra que no depende de ningún otro parámetro desconocido. Un estimador es una estadística que se usa para determinar una cantidad desconocida, y el estimado es el valor observado del estimador (evaluando la función en la muestra).
		\end{defi}
		
		El comportamiento de un estimador y su efectividad para estimar un parámetro se puede medir a través de la distribución de probabilidad del estimador, conocida como distribución muestral.
		
		\begin{defi}[Distribución muestral]
			Para una muestra $X_1,\dots, X_n$ y una estadística $T=T(X_1,\dots,X_n)$, la distribución muestral de la estadística $T$ es la distribución de probabilidad asociada a la variable aleatoria $T$. La pdf de la distribución muestral se denota como $f_T(t;\theta)$.
		
		\end{defi}
		Antes de continuar, es necesario definir el concepto de valor esperado o media, requerido para medir la eficiencia de un estimador.
		
		\begin{defi}[Valor esperado]
			Tome $X$ como una variable aleatoria con pdf $f(x)$ en $\mathbb{S}_X$. El valor esperado de la variable aleatoria $X$, denotado por $E(X)$, se define como 
			$$E(X)=\sum_{x\in\mathbb{S}_X} xf(x)$$
			cuando $X$ es una variable aleatoria discreta, y como
			$$E(X)=\int_{x\in\mathbb{S}_X} xf(x)\mathrm{d}x$$
			cuando $X$ es una variable aleatoria continua.
		\end{defi}
Cuando una estadística $T$ es usada para estimar un parámetro $\theta$, se espera que la media de dicha estadística sea cercano a $\theta$. Cuando se da la igualdad, $T$ se llama estimador imparcial del parámetro $\theta$.

	\begin{defi}[Estimador imparcial]
		Una estadística $T$ se dice estimador imparcial de un parámetro $\theta$ cuando $E(T)=\theta$, $\forall \theta\in\Theta$. Una estadística se conoce como estimador parcial de $\theta$ cuando $E(T)\neq \theta$, y la parcialidad de una estadística $T$ para estimar un parámetro $\theta$ se define como  $\mathop{Bias}(T;\theta)=E(T)-\theta.$ 
	\end{defi}
	
	En algunos casos, cuando el estimador es parcial, se puede despreciar la parcialidad tomando una muestra lo suficientemente grande. Un estimador cuya parcialidad va a cero cuando $n\rightarrow\infty$ se conoce como estimador asintóticamente imparcial.
	
	\begin{defi}[Estimador asintóticamente imparcial]
		Una estadística $T_n=T(X_1,\dots,X_n)$ se conoce como estimador asintóticamente imparcial de un parámetro $\theta$ cuando $$\lim_{n\rightarrow\infty}\mathop{Bias}(T_n;\theta)=0.$$
	\end{defi}
	
	A pesar de que es importante que un estimador se aproxime a su respectivo parámetro, el valor esperado (la media) no mide la precisión ni la exactitud del estimador $T$. Para medir la precisión se introduce el \textbf{error estándar de una estadística} como la desviación estándar de la misma, es decir, 
	$$\mathop{SE}(T):=\sqrt{\mathop{E}((T-E(T))^2)}:=\sqrt{\mathop{Var}(T)}.$$
	
	Por otro lado, para medir la exactitud de un estimador $T$ usado para estimar un parámetro $\theta$ se introduce el \textbf{error cuadrado medio} asociado a $T$.
	$$\mathop{MSE(T;\theta)}=\mathop{E}((T-\theta)^2).$$
	
	Cuando se realiza una estimación paramétrica, la información acerca del parámetro desconocido $\theta\in\Theta$ está contenida en una muestra aleatoria de tamaño $n$ seleccionada a partir de una \textit{pdf} común $f(x;\theta)$. Una estadística que contiene toda la información relevante acerca de $\theta$ en una muestra se conoce como estadística suficiente \cite{rossi_2018}.
	
	\begin{defi}[Estadística suficiente]
		Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias \textit{iid} con \textit{pdf} común $f(x;\theta)$, para $\theta\in\Theta\subseteq\mathbb{R}^d$. Un vector de estadísticas $\vec{S}(\vec{X}):=(S_1(\vec{X}),\dots, S_k(\vec{X}))$ se dice que es una estadística suficiente $k-$dimensional para un parámetro $\theta$ si y sólo si la distribución condicional de $\vec{X}$ dado $S=s$ no depende de $\theta$, para ningún valor de $s$.
	\end{defi}
%=============================================
		
		Esta última definición no es útil en la práctica para determinar la suficiencia de una estadística. Para esta labor se introduce la función de verosimilitud, que junto con el teorema de factorización de Neyman-Fisher puede determinar si una estadística es suficiente.
		\begin{defi}[Función de verosimilitud]
			Para una muestra $X_1,\dots,X_n$, la función de verosimilitud $L(\theta|\vec{X})$ es la pdf adjunta de $\vec{X}=(X_1,\dots,X_n)$, es decir,
$$L(\theta|\vec{X})=f(x_1,\dots,x_n;\theta)$$			
			La función logarítmica de verosimilitud $\ell(\theta)$ se define como el logaritmo de la función de verosimilitud.
		\end{defi}
		Cuando $X_1,\dots,X_n$ es una muestra de variables aleatorias \textit{iid}, se puede escribir la función de verosimilitud como
		$$L(\theta)=\prod_{i=1}^n f(x_i;\theta),$$
		
		\begin{theo}[Teorema de factorización de Neyman-Fisher]
			Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias iid con pdf $f(x;\theta)$, y espacio de parámetros $\Theta$. Una estadística $S(\vec{X})$ es suficiente para $\theta$ si y sólo si $L(\theta)$ se puede factorizar como
			$$L(\theta)=g(S(\vec{x});\theta)h(\vec{x}),$$
			donde $g(S(\vec{x});\theta)$ no depende de $\vec{x}=(x_1,\dots,x_n)$, excepto a través de $S(\vec{x})$, y $h(\vec{x})$ no depende de $\theta$.
		\end{theo}
		En un modelo paramétrico $f(\vec{x};\vec{\theta})$, la función de verosimilitud conecta los datos observados con dicho modelo, de tal modo que se puede hacer inferencias estadísticas sobre $\vec{\theta}$. Su importancia se resume en la ley de verosimilitud.
		
		\textbf{Ley de verosimilitud:} Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias \textit{iid} con \textit{pdf} común $f(x;\vec{\theta})$ y espacio de parámetros $\Theta$. Para $\vec{\theta}\in\Theta$, mientras mayor sea el valor de $L(\vec{\theta})$, el modelo probabilístico con parámetro $\vec{\theta}$ se ajusta más a los datos observados. Entonces, el grado con el cual la información de la muestra da soporte a un parámetro $\vec{\theta}_0\in\Theta$, en comparación con otro parámetro $\vec{\theta}_1\in\Theta$ es igual a la razón entre sus verosimilitudes
		$$\Lambda(\vec{\theta}_0,\vec{\theta}_1)=\frac{L(\vec{\theta}_0)}{L(\vec{\theta}_1)}.$$
		
		En particular, la información en la muestra coincide mejor con $\vec{\theta}_1$ que con $\vec{\theta}_0$ cuando $\Lambda<1$, y viceversa cuando $\Lambda>1.$
		
		Para encontrar el parámetro $\vec{\theta}$ para el cual la función de verosimilitud alcanza su mayor valor se introduce la función de Score.
		
		\begin{defi}[Función de Score]
			Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias con función de verosimilitud $L(\vec{\theta})$, para $\vec{\theta}\in\Theta$. Si la función de verosimilitud logarítmica $\ell(\vec{\theta})$ es diferenciable, la función de Score se define como
			
			$$\mathop{Sc}(\vec{\theta})=\nabla_{\vec{\theta}}\ \ell(\theta),$$
			de tal modo que una condición necesaria para que $\vec{\theta}\in\Theta$ sea un máximo es que $\mathop{Sc}(\theta)=\vec{0}.$
		\end{defi}
		
		\subsection{Estimación bayesiana}
		
		En la estimación paramétrica puntual bayesiana, el parámetro $\theta$ se trata como una variable aleatoria, con su propia \textit{pdf} $\pi(\theta;\lambda)$. Esta distribución recibe el nombre de distribución previa y $\lambda$ se llama hiperparámetro de la distribución previa. Cuando $\theta$ es una variable aleatoria, el modelo paramétrico $f(x;\theta)$ que genera la muestra aleatoria es la distribución condicional de $X$ dado $\theta$, por lo que la \textit{pdf} de $X$ se denotará como $f(x|\theta)$. 
		
		Las inferencias de $\theta$ en la aproximación bayesiana están basadas en la distribución de $\theta$ dados los valores observados de una muestra aleatoria $\vec{x}=(x_1,\dots,x_n)$, llamada distribución posterior, y denotada por $f(\theta | \vec{x})$ \cite{rossi_2018}. 		
		
		Usando el teorema de Bayes y el teorema de la probabilidad total, en el caso de que $\theta$ es una variable aleatoria continua,
		
		$$f(\theta | \vec{x})=\frac{f(\vec{x},\theta;\lambda)}{f_{\vec{X}}(\vec{x})}=\frac{f(\vec{x}|\theta)\pi(\theta;\lambda)}{\int_{\mathbb{S}_\theta}f(\vec{x}|\theta)\pi(\theta;\lambda)\mathrm{d}\theta}.$$
		
		De modo similar, cuando $\theta$ es una variable aleatoria discreta,
		
		$$f(\theta|\vec{x})=\frac{f(\vec{x}|\theta)\pi(\theta;\lambda)}{\sum_{\theta\in\mathbb{S}_\theta}f(\vec{x}|\theta)\pi(\theta;\lambda)}.$$
		
		La distribución posterior combina la información disponible de $\theta$ en la distribución previa y la función de verosimilitud para producir una distribución actualizada que contiene toda la información disponible de $\theta$.
		
		El siguiente teorema indica que la distribución posterior depende de la muestra $\vec{x}$ sólo bajo una estadística suficiente para $\theta$.
		
		\begin{theo}
			Si $X_1,\dots,X_n$ es una muestra de variables independientes iid con pdf común $f(x|\theta)$, $S$ es una estadística suficiente para $\theta$, y $\pi(\theta;\lambda)$ una distribución previa para $\theta$, entonces la distribución posterior de $\theta$ dado $\vec{X}$ depende de la muestra sólo a través de una estadística suficiente $S$.
		\end{theo}
		
		\begin{proof}
		La prueba se hará únicamente para el una distribución previa discreta, puesto que en el caso continuo la prueba es similar, reemplazando la sumatoria por la integral.
		
		Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias \textit{iid} con \textit{pdf} común $f(x|\theta)$, $S$ como una estadística suficiente para $\theta$, y $\pi(\theta;\lambda)$ como la distribución previa de $\theta$. Entonces, como $S$ es suficiente para $\theta$, por el teorema de factorización de Fisher-Neyman, la distribución adjunta de $X_1,\dots,X_n$ puede ser factorizada como $f(\vec{x}|\theta)=f(S;\theta)h(\vec{x})$, para algunas funciones $g$ y $h$.
		
		Entonces, la \textit{pdf} de la distribución posterior es
		
		$$f(\theta|\vec{x})=\frac{f(\vec{x}|\theta)\pi(\theta)}{\sum_{\theta\in\mathbb{S}_\theta}f(\vec{x}|\theta)\pi(\theta)}=\frac{g(S;\theta)h(\vec{x})\pi(\theta)}{\sum_{\theta\in\mathbb{S}_\theta} g(S;\theta)h(\vec{x})\pi(\theta)}=\frac{g(S;\theta)\pi(\theta)}{\sum_{\theta\in\mathbb{S}_\theta} g(S;\theta)\pi(\theta)},$$
		que es una función de $S$ y $\theta$ únicamente. Entonces, $f(\theta|\vec{x})$ depende de $\vec{X}$ sólo a través de la estadística suficiente $S$.
		\end{proof}
		
		\section{Cadenas de Markov Monte Carlo}
		La teoría comentada en la sección anterior estipula el número de muestras necesarias para tener una estadística suficiente, y da a conocer posibles funciones de costo para estimar la precisión y exactitud de un estimador dado. Por supuesto, se requiere un estimador para obtener los valores de los parámetros de un modelo probabilístico. Para ello, se usarán algoritmos relacionados con cadenas de Markov Monte Carlo (\textit{MCMC} para abreviar).
		
		Para empezar, recuerde que el teorema de Bayes estipula que, en el caso en el cual $\theta$ es una variable aleatoria continua:
		
		\begin{equation}
			P(\theta)=\frac{L(\theta)\pi(\theta)}{\int_{\theta\in\mathbb{S}_\theta}L(\theta)\pi(\theta)\mathrm{d}\theta}=\frac{L(\theta)\pi(\theta)}{Z},
		\end{equation}
		donde $P(\theta):=f(\theta|\vec{x})$ es la distribución posterior, $L(\theta)=f(\vec{x}|\theta)$ es la función de verosimilitud, $\pi(\theta)$ es la distribución previa y la constante $Z$ se conoce como evidencia.
		
		Considere ahora una función $f(\theta)$ del parámetro o parámetros del modelo que va a $\mathbb{R}$. El valor esperado de esta función sobre todo el soporte de $\theta$ es
		\begin{equation}
			\underbrace{E_P[f(\theta)]}_{\text{valor esperado respecto a }P}=\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta.
		\end{equation}
		Esta integral se puede aproximar usando rejillas. Por ejemplo, en el caso en el que el modelo tiene un único parámetro $\theta\in\mathbb{S}_\theta\subseteq\mathbb{R}$, se puede construir una partición del soporte de $\theta$ (finita o infinita). Acá se supondrá un soporte acotado, pero el razonamiento para un soporte no acotado es similar. Considere entonces la partición $\mathcal{P}=\{\theta_1<\theta_2<\dots<\theta_{n+1}\}$. Definiendo $\Delta\theta_i=\theta_{i+1}-\theta_i$ como el desplazamiento entre los elementos de la partición y $\overline{\theta_i}=\frac{\theta_{i+1}+\theta_i}{2}$ como el punto medio entre $\theta_{i+1}$ y $\theta_i$, el valor esperado de $f(\theta)$ se puede aproximar de la siguiente forma:
		\begin{equation}\label{firstAproximation}
			E_P[f(\theta)]\approx \sum_{i=1}^n f(\overline{\theta_i})P(\overline{\theta_i})\Delta \theta_i.
		\end{equation}
		
		La generalización a más dimensiones es directa: se descompone el soporte $\mathbb{S}_\theta\subseteq\mathbb{R}^N$ en $n$ cuboides $N-$dimensionales. La contribución de cada uno de estos cuboides es proporcional al producto del peso $f(\overline{\theta_i})P(\overline{\theta_i})$ (donde $\overline{\theta_i}$ es el centro geométrico del $i-$ésimo cuboide) y al volumen
		$$\Delta \theta_i=\prod_{j=1}^N \Delta \theta_{i,j},$$
		donde $\Delta\theta_{i,j}$ es el ancho del $i-$ésimo cuboide en la $j$-ésima dimensión. Así, la ecuación para el valor esperado de $f(\theta)$ tiene la misma forma que \eqref{firstAproximation}. Más aún, escribiendo el valor esperado de la siguiente forma
		$$E_P[f(\theta)]=\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta}{\underbrace{\int_{\theta\in\mathbb{S}_\theta} P(\theta)\mathrm{d}\theta}_{1}}=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)ZP(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} ZP(\theta)\mathrm{d}\theta}$$
		y tomando $\tilde{P}(\theta)=ZP(\theta)$, se puede aproximar la evidencia a través del mismo procedimiento, dado que $Z=\int_{\theta\in\mathbb{S}_\theta}\tilde{P}(\theta)\mathrm{d}\theta$:
		
		$$E_P[f(\theta)]=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)\tilde{P}(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta}\approx \frac{\sum_{i=1}^nf(\overline{\theta_i})\tilde{P}(\overline{\theta_i})\Delta \theta_i}{\sum_{i=1}^n \tilde{P}(\overline{\theta_i})\Delta \theta_i}.$$
		
		Esta sustitución a la distribución posterior no normalizada $\tilde{P}(\theta)$ es crucial para calcular valores esperados en la práctica, puesto que es posible calcular directamente $\tilde{P}(\theta)=L(\theta)\pi(\theta)$, sin conocer $Z$. En el caso de que se necesite $Z$, se puede aproximar numéricamente a través de rejillas, como se vio anteriormente.
		
		Una desventaja de la aproximación a este problema por medio de rejillas es el crecimiento exponencial de la cantidad de cubos necesarios para cubrir el soporte cuando aumenta su dimensión. Otra desventaja es que, al no conocer la forma de la distribución posterior, la contribución de cada parte de la rejilla puede ser altamente inexacta, dependiendo de su estructura. Si no se escogen bien los cuboides de la rejilla se podría terminar con muchos puntos localizados en regiones donde $\tilde{P}(\theta)$ o $f(\theta)\tilde{P}(\theta)$ son relativamente pequeños, lo que implicaría que su suma podría estar dominada por un pequeño número de puntos con pesos muy grandes. Para resolver esta desventaja, se busca incrementar la resolución de la rejilla en regiones donde la distribución posterior es grande y disminuirla en las otras regiones para evitar este efecto \cite{speagle2019conceptual}.
		
		Para resolver esta última desventaja (teóricamente), se introducirá la idea de \textbf{media pesada muestral} de un conjunto de $\{f_1,\dots,f_n\}$ observaciones con peso $\{\omega_1,\dots,\omega_n\}$ de la siguiente forma:
		\begin{equation}
			f_{mean}=\frac{\sum_{i=1}^n \omega_i f_i}{\sum_{i=1}^n\omega_i}.
\end{equation}			

	Si se toma $f_i:=f(\overline{\theta_i})$ y $\omega_i:=\tilde{P}(\overline{\theta_i})\Delta \theta_i$, se observa que el valor esperado se puede escribir de manera aproximada como una media pesada muestral. De esta forma, es necesario encontrar una forma eficiente de calcular dicha media pesada muestral para evitar la desventaja comentada. El \textbf{tamaño de muestra efectivo} $n_{eff}$ es una primera aproximación, basada en el hecho de que no todas las muestras dan la misma información. En teoría, uno puede encontrar una manera de aproximar $E_P[f(\theta)]$ de mejor o igual forma de la que se tiene en términos de una media pesada muestral de tamaño $n$ usando un número más pequeño de muestras $n_{eff}$ si se es capaz de localizarlos más eficientemente.
	
	De manera formal, se define $n_{eff}$ del siguiente modo\cite{kish_1995}:
	\begin{equation}
		n_{eff}=\frac{\left(\sum_{i=1}^n \omega_i \right)^2}{\sum_{i=1}^n\omega_i^2}.
	\end{equation}
	Intuitivamente, el mejor caso es cuando todos los pesos son iguales ($\omega_i=\omega$), donde
	$$n^{best}_{eff}=\frac{(n\omega)^2}{n\omega^2}=n,$$
	y el peor caso es cuando todo el peso está concentrado en una única muestra, ($\omega_j=\omega$, para algún $j$ y $\omega_i=0$ en otro caso):
	$$n^{worst}_{eff}=\frac{\omega^2}{\omega^2}=1.$$
	
	El mejor caso hace referencia a cuando todos los elementos de la rejilla tienen aproximadamente la misma contribución en la integral, mientras que el peor caso hace referencia a cuando la integral entera está contenida en un único cuboide de la rejilla.
	
	Se debe procurar entonces que los pesos tiendan a ser una constante. En teoría, si se conoce la distribución posterior lo suficientemente bien, para $n$ lo suficientemente grande, se podría ajustar $\Delta\theta_i$ para que los pesos $\omega_i=\tilde{P}(\overline{\theta_i})\Delta\theta_i$ sean uniformes a cierto nivel de precisión. Esta uniformidad ocurre cuando
	
	$$\Delta\theta_i \propto \frac{1}{\tilde{P}(\overline{\theta_i})}, \text{ para todo } i.$$
	
	Cuando $n\rightarrow \infty$, el espaciamiento $\Delta\theta$ cambia como función de $\theta$. Esto motiva la definición de la densidad de puntos $Q(\theta)$, conocida como \textbf{distribución propuesta}, basada en la resolución variable $\Delta\theta(\theta)$ en la rejilla infinita como función de $\theta$:
	$$Q(\theta) \propto \frac{1}{\Delta\theta(\theta)}.$$
	Usando $Q(\theta),$ se puede reescribir el valor esperado como
	$$E_P[f(\theta)]=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta) \tilde{P}(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta}=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta) \frac{\tilde{P}(\theta)}{Q(\theta)} Q(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \frac{\tilde{P}(\theta)}{Q(\theta)} Q(\theta)\mathrm{d}\theta}=\frac{E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]}{E_Q[\tilde{P}(\theta)/Q(\theta)]}.$$
	
	En palabras, la rejilla de $n$ elementos en el límite de infinita resolución se manifiesta en una nueva distribución $Q(\theta)$, con la cual se puede escribir el valor esperado $E_P[f(\theta)]$ en términos de los valores esperados $E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]$ y $E_Q[\tilde{P}(\theta)/Q(\theta)]$. La practicidad de esto radica en el hecho de que se pueden calcular estas últimos valores esperados generando una muestra aleatoria de $n$ elementos a partir de $Q(\theta)$.
	
	Como no se sabe la forma exacta de $P(\theta)$ de antemano, se desconoce cuál rejilla proveerá un estimado óptimo para $E_P[f(\theta)]$. Una de las formas de calcular este valor esperado es usando la distribución propuesta $Q(\theta)$, generando muestras a partir de ella. Esto motiva a escoger $Q(\theta)$ de manera que se puedan generar muestras de manera fácil y directa. Generando $n$ muestras $\{\theta_1,\dots,\theta_n\}$ de esta distribución, con pesos asociados $q_i$ y definiendo
	\begin{center}
	\begin{tabular}{cc}
	$f(\theta_i)=f_i,$& $\tilde{\omega_i}:=\tilde{\omega}(\theta_i)=\tilde{P}(\theta_i)/Q(\overline{\theta_i}),$
	\end{tabular}
	\end{center}
	
	el valor esperado se puede aproximar como
	
	$$E_P[f(\theta)]=\frac{E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]}{E_Q[\tilde{P}(\theta)/Q(\theta)]}\approx \frac{\sum_{i=1}^n f_i\tilde{\omega_i}q_i}{\sum_{i=1}^n \tilde{\omega_i}q_i}.$$
	
	Si además se toma $Q(\theta)$ de modo que las muestras sean \textit{iid}, los correspondientes pesos $q_i$ se reducen a $1/n,$ de manera que
	
	$$E_P[f(\theta)]\approx \frac{n^{-1}\sum_{i=1}^n f_i\tilde{\omega_i}}{n^{-1}\sum_{i=1}^n \tilde{\omega_i}}.$$
	El denominador de la última expresión es nuevamente una aproximación directa de la evidencia,
	
	$$Z=\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta\approx n^{-1}\sum_{i=1}^n \tilde{\omega_i}.$$
	De este modo, los pasos a seguir para calcular el valor esperado son los siguientes.
	\begin{enumerate}
		\item Se debe generar $n$ muestras \textit{iid} $\{\theta_1,\dots,\theta_n\}$ a partir de $Q(\theta)$.
		\item Se calculan sus correspondientes pesos $\tilde{\omega_i}=\tilde{P}(\theta_i)/Q(\theta_i).$
		\item Se estima $E_P[f(\theta)]$ aproximando $E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]$ y $E_Q[\tilde{P}(\theta)/Q(\theta)]$ a través de los pesos de las muestras.
	\end{enumerate}
	Los métodos \textit{MCMC} buscan generar muestras de tal modo que los pesos asociados $\{\tilde{\omega_1},\dots, \tilde{\omega_n}\}$ son constantes. $Q(\theta)$ juega un papel fundamental para lograr este cometido, y para ilustrar, considere los siguientes casos.
	
	\begin{itemize}
		\item Tome $Q(\theta)=Q^{unif}(\theta)$, definida sobre un cuboide de volumen $V$, de la siguiente manera
		\[   
Q^{unif}(\theta) = 
     \begin{cases}
		1/V, &\quad\text{ si }\theta\text{ está dentro del cuboide o}\\
		0	&\quad\text{ en otro caso.}
     \end{cases}
\]
Los pesos en este caso serán proporcionales a la distribución posterior:
$$\tilde{\omega_i}^{unif}=\frac{\tilde{P}(\theta_i)}{Q^{unif}(\theta_i)}=V\tilde{P}(\theta_i)\propto P(\theta_i).$$
	\item Tome $Q(\theta)=Q^{prior}(\theta)=\pi(\theta)$ como la distribución previa de $\theta$. Los pesos en este caso se pueden calcular mediante la función de verosimilitud.
	$$\tilde{\omega_i}^{prior}=\frac{\tilde{P}(\theta_i)}{Q^{prior}(\theta_i)}=\frac{ZP(\theta_i)}{\pi(\theta_i)}=\frac{L(\theta_i)\pi(\theta_i)}{\pi(\theta_i)}=L(\theta_i).$$
	\item Tome $Q(\theta)=Q^{post}(\theta)=P(\theta)$ como la distribución posterior de $\theta$, de modo que los pesos serán constantes e iguales a la evidencia $Z$:
	$$\tilde{\omega_i}^{post}=\frac{\tilde{P}(\theta_i)}{Q^{post}(\theta_i)}=\frac{ZP(\theta_i)}{P(\theta_i)}=Z.$$
	\end{itemize}
	Siguiendo la idea del último caso, si uno desea que sus pesos sean constantes, se debe procurar que $Q(\theta)$ sea lo más cercana posible a $P(\theta)$. Los modelos \textit{MCMC} buscan generar muestras con pesos proporcionales a la distribución posterior, para obtener un estimado óptimo del valor esperado.
	
	Los modelos \textit{MCMC} logran esto creando una cadena de valores de parámetros correlacionados $\{\theta_1\rightarrow\cdots\rightarrow\theta_n\}$ al cabo de $n$ iteraciones de tal modo que el número $m(\theta)$ de iteraciones hechas en cada región particular $\delta_\theta$, centrada en $\theta$ es proporcional a la densidad posterior $P(\theta)$. En otras palabras, la densidad de muestras generadas por el modelo \textit{MCMC}
	$$\rho(\theta):=\frac{m(\theta)}{n}$$
	en la posición $\theta$ integrada sobre $\delta_\theta$ es aproximadamente
	$$\int_{\theta\in\delta_\theta}P(\theta)\mathrm{d}\theta\approx \int_{\theta\in\delta_\theta}\rho(\theta)\mathrm{d}\theta\approx n^{-1}\sum_{j=1}^n\mathbb{1}[\theta_j\in\delta_\theta],$$
	donde $\mathbb{1}[\cdot]$ es la función indicadora, equivalente a 1 si la condición a la que está siendo evaluada es verdadera, y cero si es falsa. La densidad de muestras se puede aproximar de esta forma contando el número de muestras dentro de $\delta_\theta$ y normalizando por el número total de muestras $n$.
	
	Cuando $n\rightarrow\infty$, se garantiza que $\rho(\theta)\rightarrow P(\theta)$ en cualquier punto $\theta$ \cite{brooks_gelman_jones_2011}. Con una aproximación razonable de $\rho(\theta)$, se pueden usar las muestras $\{\theta_1\rightarrow\cdots\rightarrow\theta_n\}$ generadas por $\rho(\theta)$ para estimar la evidencia
	$$Z=\int_{\theta\in\mathbb{S}_\theta}\frac{\tilde{P}(\theta)}{\rho(\theta)}\rho(\theta)\mathrm{d}\theta =E_\rho[\tilde{P}(\theta)/\rho(\theta)]\approx n^{-1}\sum_{i=1}^n \frac{\tilde{P}(\theta_i)}{\rho(\theta_i)}.$$
	
	Además, como el modelo \textit{MCMC} produce una serie de $n$ muestras de la distribución posterior, el valor esperado de $f(\theta)$ se reduce a
	$$E_P[f(\theta)]\approx\frac{n^{-1}\sum_{i=1}^n f_i\tilde{\omega_i}}{n^{-1}\sum_{i=1}^n \tilde{\omega_i}}=\frac{n^{-1}\sum_{i=1}^n f_i}{n^{-1}\sum_{i=1}^n 1}=n^{-1}\sum_{i=1}^n f_i,$$
	que es la expresión del promedio aritmético de los valores $f_i=f(\theta_i).$