\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsthm,amssymb,amsfonts,latexsym}
\usepackage{amsmath}
\usepackage{ dsfont }

\usetheme{Warsaw}
\usepackage{ragged2e}
\apptocmd{\frame}{}{\justifying}{} % Allow optional arguments after frame.

\newtheorem{defi}{Definición}[section]
\newtheorem{theo}{Teorema}[section]
\newtheorem{corol}{Corolario}[section]



\setbeamertemplate{navigation symbols}{}
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}
\setbeamertemplate{caption}[numbered]
%\addtobeamertemplate{block begin}{}{\justifying}


\title[Teoría Bayesiana]{Teoría Bayesiana}
\author[Julián Jiménez-Cárdenas]{Julián Jiménez-Cárdenas$^{1}$}
\institute{$^{1}$Universidad Nacional de Colombia, Bogotá. \and \texttt{juojimenezca@unal.edu.co}}
\date{}

\usepackage{ccfonts,eulervm}
\usepackage[T1]{fontenc}
\begin{document}

	\frame{\titlepage}
	\begin{frame}[allowframebreaks]

		\tableofcontents		
	\end{frame}
	
	\section{Preliminares}
	\subsection{Espacio de Probabilidad}
	\begin{frame}[allowframebreaks]{$\sigma$-álgebra}
	\begin{defi}[Experimento Aleatorio]
		Un experimento se dice aleatorio si su resultado no se puede determinar de antemano.
	\end{defi}
	\begin{defi}[Espacio de Muestra]
		El conjunto $\Omega$ de todos los posibles resultados de un experimento aleatorio se llama espacio de muestra. Un elemento $\omega\in\Omega$ se llama resultado o muestra. $\Omega$ se dice discreto si es finito o contable.
	\end{defi}	
	\begin{defi}[$\sigma$-álgebra]
		Tome $\Omega\neq\emptyset$. Una colección $\Im$ de subconjuntos de $\Omega$ se llama $\sigma-$álgebra sobre $\Omega$ si:
		\begin{enumerate}
			\item $\Omega\in\Im$,
			\item Si $A\in\Im$, entonces $A^c\in\Im$ y,
			\item Si $A_1,A_2,\dots\in\Im$, entonces $\bigcup_{i=1}^\infty A_i\in\Im$.
		\end{enumerate}
		Los elementos de $\Im$ se llaman eventos.
	\end{defi}
	\begin{theo}
		Si $\Omega\neq\emptyset$ y $\Im_1,\Im_2,\dots$ son $\sigma-$álgebras sobre $\Omega$, entonces $\bigcap_{i=1}^\infty \Im_i$ es una $\sigma$-álgebra sobre $\Omega$.
	\end{theo}		
	
	\begin{proof}
		Como $\Omega\in\Im_j,$ para $j=1,2,\dots$, $\Omega\in\bigcap_{j=1}^\infty \Im_j$. Si $A\in\bigcap_{j=1}^\infty\Im_j$, $A\in\Im_j$, para $j=1,2,\dots$, de modo que $A^c\in\Im_j$, y $A^c\in\bigcap_{j=1}^\infty\Im_j$. Por último, si 
		$$A_1,A_2,\dots\in\bigcap_{j=1}^\infty\Im_j,$$
		para todo $j=1,2,\dots$, $A_1,A_2,\dots\in\Im_j$, de modo que
		$$\bigcup_{i=1}^\infty A_i\in\Im_j\text{ y }\bigcup_{i=1}^\infty A_i\in\bigcap_{j=1}^\infty\Im_j.$$ 
	\end{proof}
	
	\begin{defi}[$\sigma$-álgebra generada]
		Tome $\Omega\neq\emptyset$ y $\mathcal{A}$ como una colección de subconjuntos de $\Omega$. Si $\mathcal{M}:=\{\Im:\Im\text{ es una }\sigma-\text{álgebra sobre }\Omega \text{ que contiene a } \mathcal{A}\},$
		$$\sigma(\mathcal{A}):=\bigcap_{\Im\in \mathcal{M}}\Im$$
		es la $\sigma-$álgebra más pequeña sobre $\Omega$ que contiene a $\mathcal{A}$. Esta $\sigma-$álgebra se conoce como $\sigma$-álgebra generada por $\mathcal{A}$.
	\end{defi}
	
	\begin{defi}[Espacio de medida]
		Tome $\Omega\neq\emptyset$ y sea $\Im$ una $\sigma$-álgebra sobre $\Omega$. La pareja $(\Omega,\Im)$ se llama espacio de medida.
	\end{defi}
	$\emptyset$ es el evento imposible. $\Omega$ es el evento seguro y $\{\omega\}$, con $\omega\in\Omega$ es un evento simple. Decimos que el evento $A$ ocurre después de llevar a cabo el experimento aleatorio si se obtiene un resultado en $A$, esto es, $A$ ocurre si el resultado es algún $\omega\in A$.
	\begin{enumerate}
		\item El evento $A\cup B$ ocurre si y sólo si $A$ ocurre, $B$ pasa, o ambos ocurren.
		\item El evento $A\cap B$ ocurre si y sólo si $A$ y $B$ ocurren a la vez.
		\item El evento $A^c$ ocurre si y sólo si $A$ no ocurre.
		\item El evento $A-B$ ocurre si y sólo si $A$ ocurre pero $B$ no ocurre.
	\end{enumerate}
	\begin{defi}[Eventos mutuamente excluyentes]
		Dos eventos $A$ y $B$ se dicen mutuamente excluyentes si $A\cap B=\emptyset$.
	\end{defi}
	
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Espacio de probabilidad}
		\begin{defi}[Frecuencia relativa]
		Para cada evento $A$, el número $f_r(A):=\frac{n(A)}{n}$ se llama la frecuencia relativa de $A$, donde $n(A)$ indica el número de veces que ocurre $A$ en $n$ repeticiones del experimento aleatorio.
	\end{defi}
	Cuando $n\rightarrow\infty$, se puede hablar de la probabilidad de que ocurra el evento $A$, normalizada de $0$ a $1$. La formalización de este concepto se encuentra en la idea del espacio de probabilidad.
	
	\begin{defi}[Espacio de probabilidad]
		Tome $(\Omega,\Im)$ como un espacio de medida. Una función real $P$ sobre $\Im$ que satisface las siguientes condiciones:
		\begin{enumerate}
			\item $P(A)\geq0$ para todo $A\in\Im$ (no negativa),
			\item $P(\Omega)=1$ (normalizada) y,
			\item si $A_1,A_2,\dots$ son eventos mutuamente excluyentes en $\Im$, esto es, si
			$$A_i\cap A_j=\emptyset \text{ para todo }i\neq j, \text{ entonces}$$
			$$P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty P(A_i),$$
		\end{enumerate}
		se llama medida de probabilidad sobre $(\Omega,\Im)$. La tripleta $(\Omega,\Im,P)$ se llama espacio de probabilidad.
	\end{defi}
	\begin{theo}
		Si $(\Omega,\Im,P)$ es un espacio de probabilidad, entonces
		\begin{enumerate}
			\item $P(\emptyset)=\emptyset$.
			\item Si $A,B\in\Im$ y $A\cap B=\emptyset$, entonces $P(A\cup B)=P(A)+P(B).$
			\item Para todo $A\in\Im$, $P(A^c)=1-P(A).$
			\item Si $A\subseteq B$, entonces $P(A)\leq P(B)$ y $P(B-A)=P(B)-P(A).$ En particular, $P(A)\leq1$ para todo $A\in\Im$.
			\item Para todo $A,B\in\Im$, $P(A\cup B)=P(A)+P(B)-P(A\cap B)$.
			
		\end{enumerate}
	\end{theo}
	\begin{theo}
	\begin{enumerate}
	\setcounter{enumi}{5}
		\item Tome $\{A_n\}_n\subseteq\Im$ como una sucesión creciente, esto es, $A_n\subseteq A_{n+1}, \forall n\in\mathbb{N}$; entonces
			$$P\left(\lim_{m\rightarrow\infty}A_n\right)=\lim_{n\rightarrow\infty}P(A_n), \text{ donde } \lim_{n\rightarrow\infty} A_n:=\bigcup_{i=1}^\infty A_i.$$
		\item Tome $\{A_n\}_n\subseteq\Im$ como una sucesión decreciente, esto es, $A_n\supseteq A_{n+1}, \forall n\in\mathbb{N}$; entonces
			$$P\left(\lim_{m\rightarrow\infty}A_n\right)=\lim_{n\rightarrow\infty}P(A_n), \text{ donde } \lim_{n\rightarrow\infty} A_n:=\bigcap_{i=1}^\infty A_i.$$
	\end{enumerate}
	\end{theo}
	\begin{proof}
		\begin{enumerate}
			\item $1=P(\Omega\cup\emptyset\cup\emptyset\cup\cdots)=P(\Omega)+P(\emptyset)+P(\emptyset)+\cdots=1+P(\emptyset)+\cdots \implies P(\emptyset)=0.$
			\item $P(A\cup B)=P(A\cup B\cup\emptyset\cup\emptyset\cup\cdots)=P(A)+P(B).$
			\item $P(A)+P(A^c)=P(A\cup A^c)=P(\Omega)=1\implies P(A^c)=1-P(A).$
			\item Si $A\subseteq B$, $B=A\cup(B-A)$, de modo que $P(B)=P(A)+P(B-A).$ Como $P\geq0$, $P(B)\geq P(A)$ y $P(B-A)=P(B)-P(A).$ Si $B=\Omega,$ $P(A)\leq1.$
			\item Use el hecho de que $A\cup B=[A-(A\cap B)]\cup[B-(A\cap B)]\cup[A\cap B]$.			
		\end{enumerate}
		
	\end{proof}
	\begin{proof}
		\begin{enumerate}
		\setcounter{enumi}{5}
			\item Tome la sucesión $C_1=A_1,C_2=A_2-A_1,\dots,C_r=A_r-A_{r-1},\dots$. Es claro que
			$$\bigcup_{i=1}^\infty C_i=\bigcup_{i=1}^\infty A_i.$$
			Más aún, como $C_i\cap C_j=\emptyset\ \forall i\neq j,$ se sigue que
			$$P\left(\bigcup_{i=1}^\infty A_i\right)=P\left(\bigcup_{n=1}^\infty C_n\right)=\sum_{n=1}^\infty P(C_n)=\lim_{n\rightarrow\infty} \sum_{k=1}^n P(C_k)$$ $$=\lim_{n\rightarrow\infty}P\left(\bigcup_{k=1}^n C_k\right)=\lim_{n\rightarrow\infty}P\left(A_n\right).$$
			\item Tome la sucesión $\{B_n=A_n^c\}_n$ y aplique el resultado anterior.
		\end{enumerate}
	\end{proof}
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Notas}
	Aplicando el teorema anterior de forma inductiva, para algunos eventos $A_1,A_2,\dots,A_n\in\Im$:
	\begin{align*}
	P(A_1\cup A_2\cup \cdots\cup A_n)=&\sum_{i=1}^n P(A_i)-\sum_{i_1<i_2}P(A_{i_{1}}\cap A_{i_{2}})+\cdots \\    
			& +(-1)^{r+1}\sum_{i_1<i_2<	\cdots<i_r}P(A_{i_1}\cap A_{i_2}\cap\cdots\cap A_{i_r})\\
			&+\cdots+(-1)^{n+1}P(A_1\cap A_2\cap \cdots\cap A_n).
	\end{align*}
	\noindent\rule{8cm}{0.4pt}
	
	Tome $(\Omega,\Im,P)$ como un espacio de probabilidad con $\Omega$ finito o contable y $\Im =\mathbb{P}(\Omega)$. Tome $\emptyset\neq A \in\Im.$ Es claro que
	
	$$A=\bigcup_{\omega\in A} \{\omega\}, \text{ de modo que }$$
	
	$$P(A)=\sum_{\omega\in A} P(\omega), \text{ donde } P(\omega):=P(\{\omega\}).$$
	Así, $P$ queda completamente definido por $p_j:=P(\omega_j)$, donde $\omega_j\in\Omega.$ El vector $|\Omega|-$dimensional $p:=(p_1,p_2,\dots)$ satisface las siguientes condiciones:
	\begin{itemize}
		\item $p_j\geq0$ y
		\item $\sum_{j=1}^\infty p_j=1.$
	\end{itemize}
	Un vector que satisface las anteriores condiciones se llama \textbf{vector de probabilidad}.	
	
	\end{frame}
	\subsection{Probabilidad Condicional e Independencia de Eventos}
	\begin{frame}{Introducción}
		Tome $B$ como un evento cuya opción de ocurrir debe ser medida bajo la suposición de que otro evento $A$ fue observado. Si el experimento se repite $n$ veces bajo las mismas circunstancias, entonces la frecuencia relativa de $B$ bajo la condición $A$ se define como
		$$f_r(B|A):=\frac{n(A\cap B)}{n(A)}=\frac{\frac{n(A\cap B)}{n}}{\frac{n(A)}{n}}=\frac{f_r(A\cap B)}{f_r(A)}, \text{ si }n(A)>0.$$
		Esto motiva la siguiente definición
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Probabilidad Condicional}
		\begin{defi}[Probabilidad condicional]
			Tome $(\Omega, \Im, P)$ como un espacio de probabilidad. Si $A,B\in\Im$, con $P(A)>0$, entonces la probabilidad del evento $B$ bajo la condición $A$ se define como sigue
			$$P(B|A):=\frac{P(A\cap B)}{P(A).}$$
		\end{defi}
		El siguiente teorema provee algunas propiedades de la probabilidad condicional.
		\begin{theo}[Medida de probabilidad condicional]
			Tome $(\Omega, \Im, P)$ como un espacio de probabilidad y $A\in\Im$, con $P(A)>0$. Entonces:
			\begin{enumerate}
				\item $P(\cdot | A)$ es una medida de probabilidad sobre $\Omega$ centrada en $A$, esto es, $P(A|A)=1.$
				\item Si $A\cap B=\emptyset$, entonces $P(B|A)=0$.
				\item $P(B\cap C |A)=P(B|A\cap C)P(C|A)$ si $P(A\cap C)>0$.
				\item Si $A_1,A_2,\dots,A_n\in\Im$, con $P(A_1\cap A_2\cap\cdots\cap A_{n-1})>0$, entonces
				\begin{align*}
				P(A_1\cap A_2\cap\cdots\cap A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots\\ P(A_n|A_1\cap A_2\cap \cdots\cap A_{n-1}).
				\end{align*}
			\end{enumerate}
		\end{theo}
		
		\begin{proof}
			\begin{enumerate}
				\item Las tres propiedades de una medida de probabilidad deben ser verificadas.
				\begin{enumerate}
					\item Claramente, $P(B|A)\geq0$ para todo $B\in\Im$.
					\item $P(\Omega | A)=\frac{P(\Omega\cap A}{P(A)}=\frac{P(A)}{P(A)}=1.$ También se tiene que $P(A|A)=1.$
					\item Tome $A_1,A_2,\dots\in\Im$ una sucesión de conjuntos disyuntos. Entonces
					\begin{align*}
					P\left(\bigcup_{i=1}^\infty A_i | A\right)=\frac{P\left(A\cap \bigcup_{i=1}^\infty A_i\right)}{P(A)}=\frac{P\left(\bigcup_{i=1}^\infty A\cap A_i\right)}{P(A)}\\
					=\sum_{i=1}^\infty \frac{P(A\cap A_i)}{P(A)}=\sum_{i=1}^\infty P(A_i | A).
					\end{align*}
				\end{enumerate}
			\end{enumerate}
		\end{proof}
		
		\begin{proof}
			\begin{enumerate}
				\setcounter{enumi}{1}
				\item Si $A\cap B=\emptyset$, $P(B|A)=\frac{P(A\cap B)}{P(A)}=\frac{P(\emptyset)}{P(A)}=0.$
				\item $P(B\cap C|A)=\frac{P(A\cap B\cap C)}{P(A)}=\frac{P(B\cap C\cap A)}{P(A\cap C)}\frac{P(C\cap A)}{P(A)}=P(B|A\cap C)P(C| A).$
				\item $P(A_1\cap\cdots\cap A_n)=\frac{P(A_1\cap\cdots\cap A_n)}{P(A_1\cap\cdots\cap A_{n-1})}\frac{P(A_1\cap\cdots\cap A_{n-1})}{P(A_1\cap\cdots\cap A_{n-2})}\cdots\frac{P(A_1\cap A_2)}{P(A_1)}P(A_1)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_n|A_1\cap A_2\cap \cdots\cap A_{n-1}).$
			\end{enumerate}
		\end{proof}
		\end{frame}
		
		\begin{frame}[allowframebreaks]{Teorema probabilidad total}
		Los siguientes resultados son vitales para aplicaciones posteriores.
		\begin{theo}[Teorema de probabilidad total]
			Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$, esto es, $A_i\cap A_j=\emptyset, \forall i\neq j$ y $\bigcup_{i=1}^\infty A_i=\Omega,$ tal que $P(A_i)>0$, para todo $A_i\in\Im$. Entonces, para todo $B\in\Im$:
			$$P(B)=\sum_{i} P(B|A_i)P(A_i).$$
		\end{theo}
		\begin{proof}
			Observe que
			$$B=B\cap\Omega=B\cap\left( \bigcup_{i=1}^\infty A_i \right)=\bigcup_{i=1}^\infty B\cap A_i,$$
			de modo que
			$$P(B)=P\left(\bigcup_{i=1}^\infty B\cap A_i\right)=\sum_{i=1}^\infty P(B\cap A_i)=\sum_{i=1}^\infty P(B| A_i)P(A_i).$$
		\end{proof}
		\end{frame}
		
		\begin{frame}[allowframebreaks]{Regla de Bayes}
		Como corolario del teorema anterior, se obtiene un resultado conocido como \textbf{regla de Bayes}, que constituye la base para la \textbf{teoría Bayesiana}.
		\begin{corol}[Regla de Bayes]
			Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$ con $P(A_i)>0$, para todo $i$; entonces, para todo $B\in\Im$ con $P(B)>0:$
			$$P(A_i | B)=\frac{P(A_i)P(B| A_i)}{\sum_j P(B|A_j)P(A_j)}, \forall i.$$
		\end{corol}
		\begin{proof}
		$$P(A_i|B)=\frac{P(A_i\cap B)}{P(B)}=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B| A_i)}{\sum_j P(B|A_j)P(A_j)}.$$
		Con la partición $A_1=A, A_2=A^c$ se obtiene la forma usual de la regla de Bayes.
	\end{proof}
	\end{frame}
	
	\begin{frame}{Distribuciones a priori y a posteriori}
	\begin{defi}[Distribuciones a priori y a posteriori]
		Tome $A_1,A_2,\dots$ como una partición finita o contable de $\Omega$, con $P(A_i)>0$, para todo $i$. Si $P(B)>0$, con $B\in\Im$, entonces $\{P(A_n)\}_n$ se llama distribución a priori (antes de que $B$ ocurra), y $\{P(A_n|B)\}_n$ se llama distribución a posteriori (después de que $B$ ocurra).
	\end{defi}
	Algunas veces, la ocurrencia de un evento $B$ no afecta la probabilidad de un evento $A$, es decir,
	$$P(A|B)=P(A).$$
	En este caso, se dice que el evento $A$ es independiente del evento $B$. Esto motiva la siguiente definición.
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Eventos Independientes}
	\begin{defi}[Eventos independientes]
		Dos eventos $A$ y $B$ se dicen independientes si y sólo si
		
		$$P(A\cap B)=P(A)P(B).$$
		Si esta condición no se tiene, se dice que los eventos son dependientes.
	\end{defi}
	\end{frame}
	\subsection{Variables aleatorias}
	\begin{frame}[allowframebreaks]{Variables aleatorias}
	\begin{defi}[Variable aleatoria]
	Tome $(\Omega, \Im, P)$ como un espacio de probabilidad. Una variable aleatoria es un mapa $X:\Omega\rightarrow \mathbb{R}$ tal que, para todo $A\in\mathbb{B}$, $X^{-1}(A)\in\Im$, donde $\mathbb{B}$ es la $\sigma$-álgebra de Borel sobre $\mathbb{R}$ ($\sigma$-álgebra más pequeña que contiene todos los intervalos de la forma $(-\infty, a]$).
	
		El conjunto de posibles valores de $X$ es $\mathbb{S}:=\{x\in\mathbb{R}:\exists \omega\in\Omega\text{ tal que }X(\omega)=x\}$, conocido como \textbf{soporte de la variable aleatoria }$X$.
	\end{defi}
	
	Si $X$ es una variable aleatoria definida sobre un  espacio de probabilidad $(\Omega, \Im, P)$, se introduce la notación
	$$\{X\in B\}:=\{\omega\in\Omega: X(\omega)\in B\}, \text{ con }B\in\mathbb{B}.$$
	
		\begin{defi}[Variable aleatoria discreta]
		Una variable aleatoria $X$ se dice discreta cuando el soporte $\mathbb{S}$ de $X$ es un subconjunto finito o contable de $\mathbb{R}$. Para $x\in\mathbb{S}$, la función $f(x)=P(X=x)$ se llama función de densidad de probabilidad (pdf para abreviar).
	\end{defi}
	
	\begin{defi}[Variable aleatoria continua]
		Una variable aleatoria $X$ se dice continua si el soporte $\mathbb{S}$ de $X$ es la unión de uno o más intervalos y si existe una función no negativa y real $f(x)$ tal que $P(X\leq x)=\int_{-\infty}^x f(t)\mathrm{d}t.$ La función $f(x)$ se llama función de densidad de probabilidad (pdf).
	\end{defi}
	\vspace{2cm}
	Algunas propiedades de la \textit{pdf} discreta son las siguientes:
	\begin{enumerate}
		\item $f(x)\geq0, \forall x\in\mathbb{S}$ y $f(x)=0,\forall x\not\in \mathbb{S}$.
		\item $\sum_{x\in\mathbb{S}}f(x)=1$.
		\item $P(X\in B)=\sum_{x\in B} f(x).$
	\end{enumerate}
	Análogamente, para la \textit{pdf} continua:
	\begin{enumerate}
		\item $f(x)\geq0, \forall x\in\mathbb{S}$ y $f(x)=0,\forall x\not\in \mathbb{S}$.
		\item $\int_{\mathbb{S}}f(x)\mathrm{d}x=1$.
		\item $P(X\in B)=\int_{B}f(x)\mathrm{d}x.$
	\end{enumerate}
	\begin{defi}[Función de distribución acumulativa]
		La función de distribución acumulativa (CDF, para abreviar) de una variable aleatoria se define como la función $F(x)=P(X\leq x)$.
	\end{defi}
	El siguiente teorema resume algunas propiedades importantes de una \textit{CDF}. 
	
	\begin{theo}
		Si $X$ es una variable aleatoria, con \textit{CDF} $F(x)$, entonces:
		\begin{enumerate}
			\item $\lim_{x\rightarrow-\infty}F(x)=0$ y $\lim_{x\rightarrow\infty} F(x)=1.$
			\item $F(x)$ es no decreciente; esto es, $F(x)\leq F(y)$, siempre que $x\leq y$.
			\item $F(x)$ es continua por derecha.
			\item $P(a< X \leq b)=F(b)-F(a).$
		\end{enumerate}
	\end{theo}
	
		\begin{theo}
		Si $X$ es una variable aleatoria discreta con \textit{CDF} $F(x)$ y soporte $\mathbb{S}=\{x_0,x_1,\dots\}$, con $x_0<x_1<\cdots$, entonces, para $x_k\in\mathbb{S}$,
		$$f(x_k)=F(x_k)-F(x_{k-1}).$$
	\end{theo}
	
	\begin{theo}
		Para una variable aleatoria continua, $f(x)=\frac{\mathrm{d}F}{\mathrm{d}x}, \forall x\in\mathbb{R}.$
	\end{theo}
	\end{frame}
	\subsection{Vectores Aleatorios}	
	\begin{frame}[allowframebreaks]{Vectores Aleatorios}
	\begin{defi}[Vector aleatorio]
		Un vector aleatorio $\vec{X}=(X_1,X_2,\dots,X_k)$ es un vector $k-$dimensional, donde $X_1,\dots,X_k$ son variables aleatorias. Un vector aleatorio se dice discreto cuando cada una de las variables aleatorias que lo conforman son discretas, y continuo cuando son continuas.
	\end{defi}
	
	\begin{defi}[Variable aleatoria bivariada]
		Un vector aleatorio bidimensional $\vec{X}=(X_1,X_2)$ se llama variable aleatoria bivariada.
	\end{defi}
	\vspace{2cm}
	De modo similar al caso de las variables aleatorias, los vectores aleatorios tienen \textit{pdf}, un soporte y una \textit{CDF}. El soporte de un vector aleatorio $k-$dimensional es el conjunto de valores que puede tomar, denotado por $\mathbb{S}_{\vec{X}}\subseteq\mathbb{R}^k.$
	
	\begin{defi}[Función de densidad de probabilidad adjunta discreta]
		Tome $\vec{X}$ como un vector aleatorio discreto $k-$dimensional. La \textit{pdf} adjunta de $\vec{X}$ se define como
		$$f(\vec{x}):=f(x_1,x_2,\dots,x_k)=P(X_1=x_1,X_2=x_2,\dots, X_k=x_k)$$
		para $\vec{x}=(x_1,x_2,\dots,x_k)\in\mathbb{S}_{\vec{X}}.$
	\end{defi}
	\vspace{2cm}
	La \textit{pdf} adjunta discreta tiene las siguientes propiedades:
	\begin{enumerate}
		\item $0\leq f(x_1,x_2,\dots,x_k)\leq1, \forall \vec{x}\in\mathbb{S}_{\vec{X}}.$
		\item $\sum_{\vec{x}\in\mathbb{S}_{\vec{X}}}f(\vec{x})=1.$
		\item Para cualquier subconjunto $B\subseteq \mathbb{S}_{\vec{X}},\ P(\vec{X}\in B)=\sum_{\{\vec{x}\in\mathbb{S}_{\vec{X}}:\vec{x}\in B\}}f(\vec{x}).$
	\end{enumerate}
	
	\begin{defi}[Función de densidad de probabilidad adjunta continua]
		Tome $\vec{X}$ como un vector aleatorio $k$-dimensional continuo. La \textit{pdf} continua de $\vec{X}$ se define como cualquier función no negativa $f(\vec{x})$ que satisfaga las siguientes propiedades:
		\begin{enumerate}
			\item $f(x_1,\dots,x_k)>0, \forall \vec{x}\in\mathbb{S}_{\vec{X}}.$
			\item $\int_{\mathbb{S}_{\vec{X}}} f(x_1,\dots,x_k)\mathrm{d}x_1\cdots\mathrm{d}x_k=1.$
			\item Para cualquier subconjunto $B\subset\mathbb{S}_{\vec{X}},\ P(\vec{X}\in B)=\int_{B}f(x_1,\dots,x_k)\mathrm{d}x_1\cdots\mathrm{d}x_k.$
		\end{enumerate}
	\end{defi}
	
	\begin{defi}[Función de distribución acumulativa adjunta]
		Tome $\vec{X}=(X_1,X_2,\dots,X_k)$ como un vector aleatorio $k-$dimensional. La \textit{CDF} adjunta de $\vec{X}$ se define como
		$$F(x_1,x_2,\dots,x_k)=P(X_1\leq x_1, X_2\leq x_2,\dots, X_k\leq x_k)$$ $$\ \forall (x_1,\dots,x_k)\in\mathbb{R}^k.$$
	\end{defi}
	
	\begin{defi}[Función de densidad de probabilidad marginal]
		Tome $\vec{X}=(X_1,\dots,X_k)$ como un vector aleatorio $k-$dimensional. La función de densidad de probabilidad marginal de la variable aleatoria $X_i$ es, para los casos discreto y continuo:
		

		$$f_i(x_i)=\underbrace{\sum_{x_1\in\mathbb{S}_{X}}\cdots \sum_{x_k\in\mathbb{S}_{X_k}}}_{\text{quitando la suma sobre }x_i} f(x_1,\dots,x_k)$$
		$$f_i(x_i)=\underbrace{\int_{x_1\in\mathbb{S}_{X_1}}\cdots \int_{x_k\in\mathbb{S}_{X_k}}}_{\text{quitando la integral sobre }x_i} f(x_1,\dots,x_k)\prod_{n\neq i}\mathrm{d}x_n$$ 
	\end{defi}
	
	\begin{defi}[Función de densidad de probabilidad condicional]
		Tome $\vec{X}(X_1,\dots,X_k)$ como un vector aleatorio $k-$dimensional. Para un valor fijo de $x_i$, donde $f_i(x_i)>0$, la función de densidad de probabilidad condicional para $\vec{Y}|X_i$; donde $\vec{Y}$ es un vector aleatorio $(k-1)-$dimensional con todas las variables aleatorias de $\vec{X}$, a excepción de $X_i$; es
		$$f(\vec{y}|x_i)=\frac{f(x_1,\dots,x_k)}{f_i(x_i)},$$
		donde $\vec{y}\in\mathbb{S}_{\vec{Y}}.$
	\end{defi}
	\begin{defi}[Colección independiente de variables aleatorias]
	Una colección de variables aleatorias $\{ X_1,X_2,\dots, X_k\}$ se dice independiente cuando
	$$F(x_1,x_2,\dots,x_k)=\prod_{i=1}^k F_i(x_i), \forall\vec{x}\in\mathbb{R}^k,$$
	donde $F_i(x_i)$ es la \textit{CDF} marginal de la variable aleatoria $X_i$ (determinada a partir de la \textit{pdf} marginal: $F_i(x_i):=P(X_i\leq x_i)$).
	\end{defi}
	
	\begin{defi}[Colección independiente de variables aleatorias]
	También se puede definir la independencia entre variables aleatorias usando las pdf, en el sentido de que la misma colección de variables aleatorias se dice independiente cuando
	$$f(x_1,x_2,\dots,x_k)=\prod_{i=1}^k f_i(x_i), \forall\vec{x}\in\mathbb{S}_{\vec{X}},$$
	donde $f_i(x_i)$ es la pdf marginal asociada a la variable aleatoria $X_i$.
	
	\end{defi}
	
		\begin{defi}[Colección de variables aleatorias independientes idénticamente distribuidas]
		Una colección de variables aleatorias $\{ X_1,X_2,\dots, X_k\}$ se dice independiente e idénticamente distribuidas (iid, para abreviar) si y sólo si $X_1,X_2,\dots,X_k$ son variables aleatorias independientes y la pdf de cada variable aleatoria es idéntica.
	\end{defi}
	
	\end{frame}
	\section{Función de Verosimilitud}
	\subsection{Estadística}
	\begin{frame}{Estimación paramétrica}
	\begin{enumerate}
			\item Un modelo probabilístico $f(x,\theta)$, especificado con los valores de los parámetros desconocidos.
			\item Un conjunto de posibles valores de $\theta$ bajo consideración, llamado el espacio de parámetros, denotado por $\Theta$.
			\item Una muestra aleatoria de $n$ observaciones del modelo probabilístico.
			\item Un conjunto de estimadores puntuales para los valores de los parámetros desconocidos, basados en la información contenida en la muestra aleatoria.
			\item Las propiedades específicas de los estimadores que permiten evaluar la precisión y eficiencia del estimador.
		\end{enumerate}
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Muestra y Estadística}
	\begin{defi}[Muestra]
			Una colección de variables aleatorias $X_1,\dots,X_k$ se llama muestra de tamaño $n$. Una muestra de $n$ variables aleatorias independientes $X_1,\dots,X_n$ se llama muestra aleatoria.
		\end{defi}
		
		\begin{defi}[Estadística y estimador]
			Dada una muestra $X_1,X_2,\dots,X_n$, una estadística $T=T(X_1,\dots,X_n)$ es una función de la muestra que no depende de ningún otro parámetro desconocido. Un estimador es una estadística que se usa para determinar una cantidad desconocida, y el estimado es el valor observado del estimador (evaluando la función en la muestra).
		\end{defi}
		
		\begin{defi}[Distribución muestral]
			Para una muestra $X_1,\dots, X_n$ y una estadística $T=T(X_1,\dots,X_n)$, la distribución muestral de la estadística $T$ es la distribución de probabilidad asociada a la variable aleatoria $T$. La pdf de la distribución muestral se denota como $f_T(t;\theta)$.
		
		\end{defi}
		
		\begin{defi}[Valor esperado]
			Tome $X$ como una variable aleatoria con pdf $f(x)$ en $\mathbb{S}_X$. El valor esperado de la variable aleatoria $X$, denotado por $E(X)$, se define como 
			$$E(X)=\sum_{x\in\mathbb{S}_X} xf(x)$$
			cuando $X$ es una variable aleatoria discreta, y como
			$$E(X)=\int_{x\in\mathbb{S}_X} xf(x)\mathrm{d}x$$
			cuando $X$ es una variable aleatoria continua.
		\end{defi}
		
		\begin{defi}[Estimador imparcial]
		Una estadística $T$ se dice estimador imparcial de un parámetro $\theta$ cuando $E(T)=\theta$, $\forall \theta\in\Theta$. Una estadística se conoce como estimador parcial de $\theta$ cuando $E(T)\neq \theta$, y la parcialidad de una estadística $T$ para estimar un parámetro $\theta$ se define como  $\mathop{Bias}(T;\theta)=E(T)-\theta.$ 
	\end{defi}
	
	\begin{defi}[Estimador asintóticamente imparcial]
		Una estadística $T_n=T(X_1,\dots,X_n)$ se conoce como estimador asintóticamente imparcial de un parámetro $\theta$ cuando $$\lim_{n\rightarrow\infty}\mathop{Bias}(T_n;\theta)=0.$$
	\end{defi}
	
	Algunas variables útiles para determinar la precisión y exactitud del estimador son las siguientes:
	
	$$\mathop{SE}(T):=\sqrt{\mathop{E}((T-E(T))^2)}:=\sqrt{\mathop{Var}(T)}.$$
	
	$$\mathop{MSE(T;\theta)}=\mathop{E}((T-\theta)^2).$$
	
	Una estadística que contiene toda la información relevante acerca de $\theta$ en una muestra se conoce como estadística suficiente.
	
	\begin{defi}[Estadística suficiente]
		Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias \textit{iid} con \textit{pdf} común $f(x;\theta)$, para $\theta\in\Theta\subseteq\mathbb{R}^d$. Un vector de estadísticas $\vec{S}(\vec{X}):=(S_1(\vec{X}),\dots, S_k(\vec{X}))$ se dice que es una estadística suficiente $k-$dimensional para un parámetro $\theta$ si y sólo si la distribución condicional de $\vec{X}$ dado $S=s$ no depende de $\theta$, para ningún valor de $s$.
	\end{defi}
		%=================================
		\begin{defi}[Función de verosimilitud]
			Para una muestra $X_1,\dots,X_n$, la función de verosimilitud $L(\theta|\vec{X})$ es la pdf adjunta de $\vec{X}=(X_1,\dots,X_n)$, es decir,
$$L(\theta|\vec{X})=f(x_1,\dots,x_n;\theta)$$			
			La función logarítmica de verosimilitud $\ell(\theta)$ se define como el logaritmo de la función de verosimilitud.
		\end{defi}
		
		Cuando $X_1,\dots,X_n$ es una muestra de variables aleatorias \textit{iid}, se puede escribir la función de verosimilitud como
		$$L(\theta)=\prod_{i=1}^n f(x_i;\theta).$$
		
		\begin{theo}[Teorema de factorización de Neyman-Fisher]
			Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias iid con pdf $f(x;\theta)$, y espacio de parámetros $\Theta$. Una estadística $S(\vec{X})$ es suficiente para $\theta$ si y sólo si $L(\theta)$ se puede factorizar como
			$$L(\theta)=g(S(\vec{x});\theta)h(\vec{x}),$$
			donde $g(S(\vec{x});\theta)$ no depende de $\vec{x}=(x_1,\dots,x_n)$, excepto a través de $S(\vec{x})$, y $h(\vec{x})$ no depende de $\theta$.
		\end{theo}
		
		\end{frame}
		\begin{frame}[allowframebreaks]
		
		\begin{block}{Ley de verosimilitud}
		Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias \textit{iid} con \textit{pdf} común $f(x;\vec{\theta})$ y espacio de parámetros $\Theta$. Para $\vec{\theta}\in\Theta$, mientras mayor sea el valor de $L(\vec{\theta})$, el modelo probabilístico con parámetro $\vec{\theta}$ se ajusta más a los datos observados. Entonces, el grado con el cual la información de la muestra da soporte a un parámetro $\vec{\theta}_0\in\Theta$, en comparación con otro parámetro $\vec{\theta}_1\in\Theta$ es igual a la razón entre sus verosimilitudes
		$$\Lambda(\vec{\theta}_0,\vec{\theta}_1)=\frac{L(\vec{\theta}_0)}{L(\vec{\theta}_1)}.$$
		\end{block}
		
		\begin{defi}[Función de Score]
			Tome $X_1,\dots,X_n$ como una muestra de variables aleatorias con función de verosimilitud $L(\vec{\theta})$, para $\vec{\theta}\in\Theta$. Si la función de verosimilitud logarítmica $\ell(\vec{\theta})$ es diferenciable, la función de Score se define como
			
			$$\mathop{Sc}(\vec{\theta})=\nabla_{\vec{\theta}}\ \ell(\theta),$$
			de tal modo que una condición necesaria para que $\vec{\theta}\in\Theta$ sea un máximo es que $\mathop{Sc}(\theta)=\vec{0}.$
		\end{defi}
	\end{frame}
	
	\subsection{Estimación bayesiana}
	
	\begin{frame}[allowframebreaks]{Estimación bayesiana}
		En la estimación paramétrica puntual bayesiana, el parámetro $\theta$ se trata como una variable aleatoria, con su propia \textit{pdf} $\pi(\theta;\lambda)$.
		
		Las inferencias de $\theta$ en la aproximación bayesiana están basadas en la distribución de $\theta$ dados los valores observados de una muestra aleatoria $\vec{x}=(x_1,\dots,x_n)$, llamada distribución posterior, y denotada por $f(\theta | \vec{x})$.
		
		Usando el teorema de Bayes y el teorema de la probabilidad total, en el caso de que $\theta$ es una variable aleatoria continua,
		
		$$f(\theta | \vec{x})=\frac{f(\vec{x},\theta;\lambda)}{f_{\vec{X}}(\vec{x})}=\frac{f(\vec{x}|\theta)\pi(\theta;\lambda)}{\int_{\mathbb{S}_\theta}f(\vec{x}|\theta)\pi(\theta;\lambda)\mathrm{d}\theta}.$$
		
		De modo similar, cuando $\theta$ es una variable aleatoria discreta,
		
		$$f(\theta|\vec{x})=\frac{f(\vec{x}|\theta)\pi(\theta;\lambda)}{\sum_{\theta\in\mathbb{S}_\theta}f(\vec{x}|\theta)\pi(\theta;\lambda)}.$$
		
		La distribución posterior combina la información disponible de $\theta$ en la distribución previa y la función de verosimilitud para producir una distribución actualizada que contiene toda la información disponible de $\theta$.
		
El siguiente teorema indica que la distribución posterior depende de la muestra $\vec{x}$ sólo bajo una estadística suficiente para $\theta$.
		
		\begin{theo}
			Si $X_1,\dots,X_n$ es una muestra de variables independientes iid con pdf común $f(x|\theta)$, $S$ es una estadística suficiente para $\theta$, y $\pi(\theta;\lambda)$ una distribución previa para $\theta$, entonces la distribución posterior de $\theta$ dado $\vec{X}$ depende de la muestra sólo a través de una estadística suficiente $S$.
		\end{theo}
	\end{frame}
	
	\section{Cadenas de Markov Monte Carlo}
	\subsection{Introducción}
	\begin{frame}{Teorema de Bayes}
	El teorema de Bayes estipula que
	\begin{equation}
			P(\theta)=\frac{L(\theta)\pi(\theta)}{\int_{\theta\in\mathbb{S}_\theta}L(\theta)\pi(\theta)\mathrm{d}\theta}=\frac{L(\theta)\pi(\theta)}{Z},
		\end{equation}
		donde $P(\theta):=f(\theta|\vec{x})$ es la distribución posterior, $L(\theta)=f(\vec{x}|\theta)$ es la función de verosimilitud, $\pi(\theta)$ es la distribución previa y la constante $Z$ se conoce como evidencia.
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Aproximación del valor esperado}
	Considere ahora una función $f(\theta)$ del parámetro o parámetros del modelo que va a $\mathbb{R}$. El valor esperado de esta función sobre todo el soporte de $\theta$ es
		\begin{equation}
			\underbrace{E_P[f(\theta)]}_{\text{valor esperado respecto a }P}=\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta.
		\end{equation}
		
		Considerando una partición $\mathcal{P}=\{\theta_1<\theta_2<\dots<\theta_{n+1}\}$ (con $\mathbb{S}_\theta\subseteq\mathbb{R}$). Definiendo $\Delta\theta_i=\theta_{i+1}-\theta_i$ como el desplazamiento entre los elementos de la partición y $\overline{\theta_i}=\frac{\theta_{i+1}+\theta_i}{2}$ como el punto medio entre $\theta_{i+1}$ y $\theta_i$, el valor esperado de $f(\theta)$ se puede aproximar de la siguiente forma:
		\begin{equation}\label{firstAproximation}
			E_P[f(\theta)]\approx \sum_{i=1}^n f(\overline{\theta_i})P(\overline{\theta_i})\Delta \theta_i.
		\end{equation}
		
		La generalización a más dimensiones es directa: se descompone el soporte $\mathbb{S}_\theta\subseteq\mathbb{R}^N$ en $n$ cuboides $N-$dimensionales. La contribución de cada uno de estos cuboides es proporcional al producto del peso $f(\overline{\theta_i})P(\overline{\theta_i})$ (donde $\overline{\theta_i}$ es el centro geométrico del $i-$ésimo cuboide) y al volumen
		$$\Delta \theta_i=\prod_{j=1}^N \Delta \theta_{i,j},$$
		donde $\Delta\theta_{i,j}$ es el ancho del $i-$ésimo cuboide en la $j$-ésima dimensión.
		
		Escribiendo el valor esperado de la siguiente forma
		$$E_P[f(\theta)]=\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)P(\theta)\mathrm{d}\theta}{\underbrace{\int_{\theta\in\mathbb{S}_\theta} P(\theta)\mathrm{d}\theta}_{1}}$$ $$=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)ZP(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} ZP(\theta)\mathrm{d}\theta}$$
		y tomando $\tilde{P}(\theta)=ZP(\theta)$, se puede aproximar la evidencia a través del mismo procedimiento, dado que $Z=\int_{\theta\in\mathbb{S}_\theta}\tilde{P}(\theta)\mathrm{d}\theta$:
		
		$$E_P[f(\theta)]=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta)\tilde{P}(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta}\approx \frac{\sum_{i=1}^nf(\overline{\theta_i})\tilde{P}(\overline{\theta_i})\Delta \theta_i}{\sum_{i=1}^n \tilde{P}(\overline{\theta_i})\Delta \theta_i}.$$
		Algunas desventajas de este procedimiento son:
		\begin{enumerate}
			\item El crecimiento exponencial de la cantidad de cubos necesarios para cubrir el soporte cuando aumenta su dimensión.
			\item Los pesos arbitrarios seleccionados al momento de crear la rejilla para aproximar la integral.
		\end{enumerate}
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Media pesada muestral}
	La \textbf{media pesada muestral} de un conjunto de $\{f_1,\dots,f_n\}$ observaciones con peso $\{\omega_1,\dots,\omega_n\}$ de la siguiente forma:
		\begin{equation}
			f_{mean}=\frac{\sum_{i=1}^n \omega_i f_i}{\sum_{i=1}^n\omega_i}.
\end{equation}			

Si se toma $f_i:=f(\overline{\theta_i})$ y $\omega_i:=\tilde{P}(\overline{\theta_i})\Delta \theta_i$, se observa que el valor esperado se puede escribir de manera aproximada como una media pesada muestral.

El \textbf{tamaño de muestra efectivo} $n_{eff}$ es una herramienta para calcular de manera eficiente la media pesada muestral, basada en el hecho de que no todas las muestras dan la misma información. 

De manera formal, se define $n_{eff}$ del siguiente modo\cite{kish_1995}:
	\begin{equation}
		n_{eff}=\frac{\left(\sum_{i=1}^n \omega_i \right)^2}{\sum_{i=1}^n\omega_i^2}.
	\end{equation}
	Intuitivamente, el mejor caso es cuando todos los pesos son iguales ($\omega_i=\omega$), donde
	$$n^{best}_{eff}=\frac{(n\omega)^2}{n\omega^2}=n,$$
	y el peor caso es cuando todo el peso está concentrado en una única muestra, ($\omega_j=\omega$, para algún $j$ y $\omega_i=0$ en otro caso):
	$$n^{worst}_{eff}=\frac{\omega^2}{\omega^2}=1.$$
	
	\end{frame}
	
	\begin{frame}[allowframebreaks]{Distribución propuesta}
	Se debe procurar entonces que los pesos tiendan a ser una constante. En teoría, si se conoce la distribución posterior lo suficientemente bien, para $n$ lo suficientemente grande, se podría ajustar $\Delta\theta_i$ para que los pesos $\omega_i=\tilde{P}(\overline{\theta_i})\Delta\theta_i$ sean uniformes a cierto nivel de precisión. Esta uniformidad ocurre cuando
	
	$$\Delta\theta_i \propto \frac{1}{\tilde{P}(\overline{\theta_i})}, \text{ para todo } i.$$
	
	Cuando $n\rightarrow \infty$, el espaciamiento $\Delta\theta$ cambia como función de $\theta$. Esto motiva la definición de la densidad de puntos $Q(\theta)$, conocida como \textbf{distribución propuesta}, basada en la resolución variable $\Delta\theta(\theta)$ en la rejilla infinita como función de $\theta$:
	$$Q(\theta) \propto \frac{1}{\Delta\theta(\theta)}.$$
	
	Usando $Q(\theta),$ se puede reescribir el valor esperado como
	$$E_P[f(\theta)]=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta) \tilde{P}(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta}=\frac{\int_{\theta\in\mathbb{S}_\theta} f(\theta) \frac{\tilde{P}(\theta)}{Q(\theta)} Q(\theta)\mathrm{d}\theta}{\int_{\theta\in\mathbb{S}_\theta} \frac{\tilde{P}(\theta)}{Q(\theta)} Q(\theta)\mathrm{d}\theta}$$$$E_P[f(\theta)]=\frac{E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]}{E_Q[\tilde{P}(\theta)/Q(\theta)]}.$$
	
	En palabras, la rejilla de $n$ elementos en el límite de infinita resolución se manifiesta en una nueva distribución $Q(\theta)$, con la cual se puede escribir el valor esperado $E_P[f(\theta)]$ en términos de los valores esperados $E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]$ y $E_Q[\tilde{P}(\theta)/Q(\theta)]$. Estos valores esperados generando una muestra aleatoria de $n$ elementos a partir de $Q(\theta)$.
	
	\vspace{5cm}
	
	Esto motiva a escoger $Q(\theta)$ de manera que se puedan generar muestras de manera fácil y directa. Generando $n$ muestras $\{\theta_1,\dots,\theta_n\}$ de esta distribución, con pesos asociados $q_i$ y definiendo
	\begin{center}
	\begin{tabular}{cc}
	$f(\theta_i)=f_i,$& $\tilde{\omega_i}:=\tilde{\omega}(\theta_i)=\tilde{P}(\theta_i)/Q(\theta_i),$
	\end{tabular}
	\end{center}
	
	el valor esperado se puede aproximar como
	
	$$E_P[f(\theta)]=\frac{E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]}{E_Q[\tilde{P}(\theta)/Q(\theta)]}\approx \frac{\sum_{i=1}^n f_i\tilde{\omega_i}q_i}{\sum_{i=1}^n \tilde{\omega_i}q_i}.$$
	
	Si además se toma $Q(\theta)$ de modo que las muestras sean \textit{iid}, los correspondientes pesos $q_i$ se reducen a $1/n,$ de manera que
	
	$$E_P[f(\theta)]\approx \frac{n^{-1}\sum_{i=1}^n f_i\tilde{\omega_i}}{n^{-1}\sum_{i=1}^n \tilde{\omega_i}}.$$
	El denominador de la última expresión es nuevamente una aproximación directa de la evidencia,
	
	$$Z=\int_{\theta\in\mathbb{S}_\theta} \tilde{P}(\theta)\mathrm{d}\theta\approx n^{-1}\sum_{i=1}^n \tilde{\omega_i}.$$
	\end{frame}
	\begin{frame}[allowframebreaks]{Cadenas de Markov}
	Los pasos a seguir para calcular el valor esperado son los siguientes.
	\begin{enumerate}
		\item Se debe generar $n$ muestras \textit{iid} $\{\theta_1,\dots,\theta_n\}$ a partir de $Q(\theta)$.
		\item Se calculan sus correspondientes pesos $\tilde{\omega_i}=\tilde{P}(\theta_i)/Q(\theta_i).$
		\item Se estima $E_P[f(\theta)]$ aproximando $E_Q[f(\theta)\tilde{P}(\theta)/Q(\theta)]$ y $E_Q[\tilde{P}(\theta)/Q(\theta)]$ a través de los pesos de las muestras.
	\end{enumerate}
	
	Los métodos \textit{MCMC} buscan generar muestras de tal modo que los pesos asociados $\{\tilde{\omega_1},\dots, \tilde{\omega_n}\}$ sean constantes. $Q(\theta)$ juega un papel fundamental para lograr este cometido, y para ilustrar, considere los siguientes casos.
	
	\begin{itemize}
		\item Tome $Q(\theta)=Q^{unif}(\theta)$, definida sobre un cuboide de volumen $V$, de la siguiente manera
		\[   
Q^{unif}(\theta) = 
     \begin{cases}
		1/V, &\quad\text{ si }\theta\text{ está dentro del cuboide o}\\
		0	&\quad\text{ en otro caso.}
     \end{cases}
\]
Los pesos en este caso serán proporcionales a la distribución posterior:
$$\tilde{\omega_i}^{unif}=\frac{\tilde{P}(\theta_i)}{Q^{unif}(\theta_i)}=V\tilde{P}(\theta_i)\propto P(\theta_i).$$
	\item Tome $Q(\theta)=Q^{prior}(\theta)=\pi(\theta)$ como la distribución previa de $\theta$. Los pesos en este caso se pueden calcular mediante la función de verosimilitud.
	$$\tilde{\omega_i}^{prior}=\frac{\tilde{P}(\theta_i)}{Q^{prior}(\theta_i)}=\frac{ZP(\theta_i)}{\pi(\theta_i)}=\frac{L(\theta_i)\pi(\theta_i)}{\pi(\theta_i)}=L(\theta_i).$$
	\item Tome $Q(\theta)=Q^{post}(\theta)=P(\theta)$ como la distribución posterior de $\theta$, de modo que los pesos serán constantes e iguales a la evidencia $Z$:
	$$\tilde{\omega_i}^{post}=\frac{\tilde{P}(\theta_i)}{Q^{post}(\theta_i)}=\frac{ZP(\theta_i)}{P(\theta_i)}=Z.$$
	\end{itemize}
	Siguiendo la idea del último caso, si uno desea que sus pesos sean constantes, se debe procurar que $Q(\theta)$ sea lo más cercana posible a $P(\theta)$. Los modelos \textit{MCMC} buscan generar muestras con pesos proporcionales a la distribución posterior, para obtener un estimado óptimo del valor esperado.
	
	Los modelos \textit{MCMC} logran esto creando una cadena de valores de parámetros correlacionados $\{\theta_1\rightarrow\cdots\rightarrow\theta_n\}$ al cabo de $n$ iteraciones de tal modo que el número $m(\theta)$ de iteraciones hechas en cada región particular $\delta_\theta$, centrada en $\theta$ es proporcional a la densidad posterior $P(\theta)$. En otras palabras, la densidad de muestras generadas por el modelo \textit{MCMC}
	$$\rho(\theta):=\frac{m(\theta)}{n}$$
	en la posición $\theta$ integrada sobre $\delta_\theta$ es aproximadamente
	$$\int_{\theta\in\delta_\theta}P(\theta)\mathrm{d}\theta\approx \int_{\theta\in\delta_\theta}\rho(\theta)\mathrm{d}\theta\approx n^{-1}\sum_{j=1}^n\mathbb{1}[\theta_j\in\delta_\theta],$$
	donde $\mathbb{1}[\cdot]$ es la función indicadora, equivalente a 1 si la condición a la que está siendo evaluada es verdadera, y cero si es falsa.
	
	Cuando $n\rightarrow\infty$, se garantiza que $\rho(\theta)\rightarrow P(\theta)$ en cualquier punto $\theta$ \cite{brooks_gelman_jones_2011}. Con una aproximación razonable de $\rho(\theta)$, se pueden usar las muestras $\{\theta_1\rightarrow\cdots\rightarrow\theta_n\}$ generadas por $\rho(\theta)$ para estimar la evidencia
	$$Z=\int_{\theta\in\mathbb{S}_\theta}\frac{\tilde{P}(\theta)}{\rho(\theta)}\rho(\theta)\mathrm{d}\theta =E_\rho[\tilde{P}(\theta)/\rho(\theta)]\approx n^{-1}\sum_{i=1}^n \frac{\tilde{P}(\theta_i)}{\rho(\theta_i)}.$$
	
	Además, como el modelo \textit{MCMC} produce una serie de $n$ muestras de la distribución posterior, el valor esperado de $f(\theta)$ se reduce a
	$$E_P[f(\theta)]\approx\frac{n^{-1}\sum_{i=1}^n f_i\tilde{\omega_i}}{n^{-1}\sum_{i=1}^n \tilde{\omega_i}}=\frac{n^{-1}\sum_{i=1}^n f_i}{n^{-1}\sum_{i=1}^n 1}=n^{-1}\sum_{i=1}^n f_i,$$
	que es la expresión del promedio aritmético de los valores $f_i=f(\theta_i).$
	\end{frame}
	
	\subsection{Algoritmo de Metropolis-Hastings}
	\begin{frame}[allowframebreaks]{Metropolis-Hastings}
Se desea generar muestras $\theta_i\rightarrow\theta_{i+1}$ de modo que la distribución de las muestras finales $\rho(\theta)$ sea estacionaria cuando $n\rightarrow\infty$ (que converja) y sea igual a $P(\theta)$. La primera condición se puede satisfacer usando el \textbf{balance detallado}, que refiere a la idea de que la probabilidad sea conservada cuando uno se mueve de una posición a otra (es decir, el proceso es reversible). Formalmente, esto implica que
	$$M(\theta_{i+1}|\theta_i)M(\theta_i)=M(\theta_{i+1},\theta_{i})=M(\theta_i|\theta_{i+1})M(\theta_{i+1}),$$
	donde $M(\theta_{i+1}|\theta_i)$ es la probabilidad de moverse de $\theta_i$ a $\theta_{i+1}$ y $M(\theta_{i+1}|\theta_i)$ es la probabilidad de moverse de $\theta_{i+1}$ a $\theta_i$. Reescribiendo esta última igualdad:
	
	\begin{equation}\label{eq:detailedBalance}
	\frac{M(\theta_{i+1}|\theta_i)}{M(\theta_{i}|\theta_{i+1})}=\frac{M(\theta_{i+1})}{M(\theta_i)}=\frac{P(\theta_{i+1})}{P(\theta_i)}.
	\end{equation}
	
	Se propone una nueva posición $\theta_i\rightarrow\theta'_{i+1}$ usando la distribución propuesta $Q(\theta'_{i+1}|\theta_i)$. Se decide si aceptar ($\theta_{i+1}=\theta'_{i+1}$) o rechazar ($\theta_{i+1}=\theta_{i}$) esta nueva posición con una \textbf{probabilidad de transición} $T(\theta'_{i+1}|\theta_i)$. Combinando ambas distribuciones, se obtiene la probabilidad de moverse a una nueva posición
	$$M(\theta_{i+1}|\theta_i)=Q(\theta_{i+1}|\theta_i)T(\theta_{i+1}|\theta_i).$$
	
	Para encontrar $T$, se hace uso de la condición de balance detallado \eqref{eq:detailedBalance}.
	$$\frac{T(\theta_{i+1}|\theta_i)}{T(\theta_{i}|\theta_{i+1})}=\frac{P(\theta_{i+1})}{P(\theta_i)}\frac{Q(\theta_{i}|\theta_{i+1})}{Q(\theta_{i+1}|\theta_i)}.$$
	El criterio de Metropolis \cite{doi:10.1063/1.1699114}:
	$$T(\theta_{i+1}|\theta_i)=\min \left[ 1, \frac{P(\theta_{i+1})}{P(\theta_i)}\frac{Q(\theta_{i}|\theta_{i+1})}{Q(\theta_{i+1}|\theta_i)} \right]$$
	
	satisface esta condición.
	
	\end{frame}
	\begin{frame}{Algoritmo de Metropolis-Hastings}
	El algoritmo para generar la muestra es el siguiente:
	\begin{enumerate}
		\item Se propone una nueva posición $\theta_i\rightarrow\theta'_{i+1}$, generando una muestra de la distribución propuesta $Q(\theta'_{i+1}|\theta_i)$.
		\item Se calcula la probabilidad de transición $T(\theta'_{i+1}|\theta_i)$.
		\item Se genera un número aleatorio $u_{i+1}$ a partir de una distribución uniforme entre 0 y 1.
		\item Si $u_{i+1}\leq T(\theta'_{i+1}|\theta_i)$, se acepta el movimiento y se toma $\theta_{i+1}=\theta_i$. Si $u_{i+1}> T(\theta'_{i+1}|\theta_i)$, se rechaza el movimiento y se toma $\theta_{i+1}=\theta_i$.
		\item Se incrementa $i=i+1$ y se repite el proceso.
	\end{enumerate}
	
	\end{frame}
	\section{Referencias}
	\begin{frame}[allowframebreaks]
	\frametitle{Referencias}

	\nocite{*}
	\bibliographystyle{plain}
	\bibliography{referenciasExpo}

	\end{frame}
	%=======================================================
	\end{document}